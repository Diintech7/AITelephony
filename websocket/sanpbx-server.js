/* eslint-disable no-console */
const WebSocket = require("ws")
const { WebSocket: DGWebSocket } = require("ws")
const fetch = require("node-fetch")
const fs = require("fs")

// ENV
const ELEVEN_API_KEY = process.env.ELEVEN_API_KEY
const ELEVEN_VOICE_ID = process.env.ELEVEN_VOICE_ID || "kdmDKE6EkgrWrrykO9Qt"
const DEEPGRAM_API_KEY = process.env.DEEPGRAM_API_KEY
const OPENAI_API_KEY = process.env.OPENAI_API_KEY
const SARVAM_API_KEY = process.env.SARVAM_API_KEY

if (!SARVAM_API_KEY || !DEEPGRAM_API_KEY || !OPENAI_API_KEY) {
  console.error("‚ùå Missing API keys in .env (Sarvam / Deepgram / OpenAI)")
  process.exit(1)
}

const ts = () => new Date().toISOString()
const wait = (ms) => new Promise((res) => setTimeout(res, ms))

// -------- Enhanced Base64 Validation --------
function validateBase64Audio(base64String, label = "AUDIO") {
  try {
    if (!base64String || typeof base64String !== 'string') {
      console.log(`[${label}] ‚ùå Invalid base64 string: ${typeof base64String}`)
      return false
    }
    
    // Check if it's valid base64
    const buffer = Buffer.from(base64String, 'base64')
    if (buffer.length === 0) {
      console.log(`[${label}] ‚ùå Empty buffer from base64`)
      return false
    }
    
    // Check if buffer length is even (for 16-bit PCM)
    if (buffer.length % 2 !== 0) {
      console.log(`[${label}] ‚ö†Ô∏è Odd buffer length: ${buffer.length} bytes`)
    }
    
    console.log(`[${label}] ‚úÖ Valid base64: ${base64String.length} chars -> ${buffer.length} bytes`)
    
    // Sample first few values
    const samples = []
    for (let i = 0; i < Math.min(8, buffer.length - 1); i += 2) {
      samples.push(buffer.readInt16LE(i))
    }
    console.log(`[${label}] First samples: [${samples.join(', ')}]`)
    
    return true
  } catch (err) {
    console.log(`[${label}] ‚ùå Base64 validation error: ${err.message}`)
    return false
  }
}

// -------- Logging utilities --------
function logFullData(label, data) {
  console.log(`\n=== ${label} ===`)
  console.log(JSON.stringify(data, null, 2))
  console.log(`=== END ${label} ===\n`)
}

function logAudioSample(label, buffer, maxBytes = 32) {
  if (!buffer || buffer.length === 0) {
    console.log(`[${label}] Empty buffer`)
    return
  }
  
  const sample = buffer.slice(0, Math.min(maxBytes, buffer.length))
  const hex = sample.toString('hex').match(/.{1,2}/g).join(' ')
  const decimal = Array.from(sample).join(', ')
  
  console.log(`[${label}] First ${sample.length} bytes:`)
  console.log(`  HEX: ${hex}`)
  console.log(`  DEC: ${decimal}`)
  
  if (buffer.length >= 2) {
    const int16samples = []
    for (let i = 0; i < Math.min(8, buffer.length - 1); i += 2) {
      int16samples.push(buffer.readInt16LE(i))
    }
    console.log(`  INT16LE: [${int16samples.join(', ')}]`)
  }
}

// -------- Audio Format Conversion --------
function resampleAudio(buffer, fromRate, toRate) {
  if (fromRate === toRate) return buffer

  const ratio = fromRate / toRate
  const newLength = Math.floor(buffer.length / 2 / ratio) * 2
  const result = Buffer.alloc(newLength)

  for (let i = 0; i < newLength; i += 2) {
    const sourceIndex = Math.floor((i / 2) * ratio) * 2
    if (sourceIndex < buffer.length - 1) {
      result[i] = buffer[sourceIndex]
      result[i + 1] = buffer[sourceIndex + 1]
    }
  }

  console.log(`[AUDIO-RESAMPLE] ${fromRate}Hz -> ${toRate}Hz: ${buffer.length} -> ${newLength} bytes`)
  logAudioSample("RESAMPLED", result, 16)
  return result
}

// -------- PCM utils --------
function calculateChunkSize(sampleRate, durationMs = 20, channels = 1) {
  // Calculate bytes for the given duration
  const samplesPerMs = sampleRate / 1000
  const samplesForDuration = Math.round(samplesPerMs * durationMs)
  const bytes = samplesForDuration * 2 * channels // 16-bit = 2 bytes per sample
  
  console.log(`[PCM-CALC] Rate: ${sampleRate}Hz, Duration: ${durationMs}ms, Channels: ${channels}`)
  console.log(`[PCM-CALC] Samples per chunk: ${samplesForDuration}, Bytes per chunk: ${bytes}`)
  return bytes
}

// -------- CRITICAL: Audio Streaming with SanIPPBX Format --------
async function streamAudioToCallRealtime({ ws, streamId, channelId, callId, pcmBuffer, sampleRate, channels, callDirection = "Incoming" }) {
  if (!ws || ws.readyState !== WebSocket.OPEN) {
    console.error("[STREAM-ERROR] ‚ùå WebSocket not open")
    return 0
  }

  if (!streamId || !channelId || !callId) {
    console.error("[STREAM-ERROR] ‚ùå Missing required IDs:", { streamId, channelId, callId })
    return 0
  }

  if (!pcmBuffer || pcmBuffer.length === 0) {
    console.error("[STREAM-ERROR] ‚ùå Empty PCM buffer")
    return 0
  }

  // Use 20ms chunks like the incoming format
  const chunkDurationMs = 20
  const chunkBytes = calculateChunkSize(sampleRate, chunkDurationMs, channels)
  const totalChunks = Math.ceil(pcmBuffer.length / chunkBytes)
  let sentCount = 0
  
  console.log(`\n[STREAM-START] ${ts()}`)
  console.log(`  üîä Total chunks: ${totalChunks}`)
  console.log(`  üì¶ Chunk size: ${chunkBytes} bytes (${chunkDurationMs}ms)`)
  console.log(`  üéµ Sample rate: ${sampleRate}Hz`)
  console.log(`  üîä Channels: ${channels}`)
  console.log(`  üíæ Buffer size: ${pcmBuffer.length} bytes`)
  console.log(`  ‚è±Ô∏è Expected duration: ${(pcmBuffer.length / (sampleRate * 2 * channels)).toFixed(2)}s`)
  
  logAudioSample("STREAM-INPUT", pcmBuffer, 32)

  for (let pos = 0; pos < pcmBuffer.length && ws.readyState === WebSocket.OPEN; pos += chunkBytes) {
    const slice = pcmBuffer.slice(pos, pos + chunkBytes)
    
    // Ensure chunk is exactly the right size (pad with zeros if needed)
    const paddedSlice = slice.length < chunkBytes ? 
      Buffer.concat([slice, Buffer.alloc(chunkBytes - slice.length, 0)]) : 
      slice

    // CRITICAL: Convert to base64 and validate
    const base64Payload = paddedSlice.toString("base64")
    
    if (!validateBase64Audio(base64Payload, `CHUNK-${sentCount}`)) {
      console.error(`[STREAM-ERROR] ‚ùå Invalid base64 for chunk ${sentCount}`)
      continue
    }
    
    // Match the exact format from SanIPPBX incoming media
    const mediaEvent = {
      event: "media",
      payload: base64Payload,  // Direct payload field, not nested under media
      chunk: sentCount + 1,    // Chunk number starting from 1
      chunk_durn_ms: chunkDurationMs,
      channelId: channelId,
      callId: callId,
      streamId: streamId,
      callDirection: callDirection,
      timestamp: new Date().toISOString().replace('T', ' ').substring(0, 19)
    }
    
    // Enhanced logging for first few chunks
    if (sentCount < 5) {
      console.log(`\n[CHUNK-${sentCount}] üì¶ Processing chunk`)
      console.log(`  Raw size: ${slice.length}/${chunkBytes} bytes`)
      console.log(`  Padded size: ${paddedSlice.length} bytes`)
      console.log(`  Base64 length: ${base64Payload.length} characters`)
      console.log(`  StreamId: ${streamId}`)
      
      logAudioSample(`CHUNK-${sentCount}`, paddedSlice, 16)
      
      // Log the exact JSON being sent
      console.log(`[CHUNK-${sentCount}] üì§ Media event JSON:`)
      console.log(JSON.stringify(mediaEvent, null, 2))
      
      // Verify the base64 can be decoded back
      const testDecode = Buffer.from(base64Payload, 'base64')
      console.log(`[CHUNK-${sentCount}] üîç Base64 decode test: ${testDecode.length} bytes`)
    }
    
    try {
      const jsonString = JSON.stringify(mediaEvent)
      console.log(`[CHUNK-${sentCount}] üì° Sending ${jsonString.length} bytes to WebSocket`)
      
      ws.send(jsonString)
      sentCount++
      
      console.log(`[CHUNK-${sentCount-1}] ‚úÖ Sent successfully`)
      
    } catch (err) {
      console.error(`[STREAM-ERROR] ‚ùå Failed to send chunk ${sentCount}: ${err.message}`)
      console.error(`[STREAM-ERROR] WebSocket state: ${ws.readyState}`)
      break
    }
    
    // Progress logging
    if (sentCount % 20 === 0 || sentCount === totalChunks) {
      console.log(`[STREAM-PROGRESS] ${ts()} - üîä Sent ${sentCount}/${totalChunks} chunks`)
    }
    
    // Critical timing: 20ms delay between chunks to match chunk duration
    if (sentCount < totalChunks) {
      await wait(chunkDurationMs)
    }
  }
  
  console.log(`[STREAM-COMPLETE] ${ts()} - üéØ Final count: ${sentCount}/${totalChunks} chunks sent`)
  
  // Final validation
  if (sentCount === totalChunks) {
    console.log(`[STREAM-COMPLETE] ‚úÖ All chunks sent successfully`)
  } else {
    console.log(`[STREAM-COMPLETE] ‚ö†Ô∏è Incomplete: ${sentCount}/${totalChunks} chunks sent`)
  }
  
  return sentCount
}

// -------- Enhanced Test Audio Generation --------
async function testAudioGeneration(sampleRate, duration = 4.0) {
  console.log(`\n[TEST-AUDIO] üéµ Generating test tone for ${sampleRate}Hz`)
  
  const frequency = 800 // 800Hz tone - easily audible
  const samples = Math.floor(sampleRate * duration)
  const buffer = Buffer.alloc(samples * 2)
  
  console.log(`[TEST-AUDIO] üîä Generating ${samples} samples for ${duration}s`)
  
  for (let i = 0; i < samples; i++) {
    // Generate a clear sine wave with good amplitude
    const amplitude = 16000 // Strong signal
    const sample = Math.sin(2 * Math.PI * frequency * i / sampleRate) * amplitude
    
    // Add envelope to prevent clicks (fade in/out)
    const fadeLength = Math.floor(sampleRate * 0.1) // 100ms fade
    let envelope = 1.0
    
    if (i < fadeLength) {
      envelope = i / fadeLength
    } else if (i > samples - fadeLength) {
      envelope = (samples - i) / fadeLength
    }
    
    const finalSample = Math.round(sample * envelope)
    buffer.writeInt16LE(finalSample, i * 2)
  }
  
  console.log(`[TEST-AUDIO] ‚úÖ Generated ${buffer.length} bytes (${duration}s @ ${frequency}Hz)`)
  logAudioSample("TEST-AUDIO", buffer, 32)
  
  // Validate the test audio
  const testBase64 = buffer.toString('base64')
  validateBase64Audio(testBase64, "TEST-AUDIO-BASE64")
  
  // Save test audio for debugging
  try {
    const filename = `/tmp/debug_test_${sampleRate}hz_${Date.now()}.pcm`
    fs.writeFileSync(filename, buffer)
    console.log(`[TEST-AUDIO] üíæ Saved test audio to ${filename}`)
  } catch (err) {
    console.log(`[TEST-AUDIO] ‚ö†Ô∏è Could not save test file: ${err.message}`)
  }
  
  return buffer
}

// -------- Enhanced ElevenLabs TTS --------
async function synthesizeAndStreamAudio({ text, ws, streamId, channelId, callId, sampleRate, channels, callDirection }) {
  console.log(`\n[TTS-START] ${ts()}`)
  console.log(`  üìù Text: "${text}"`)
  console.log(`  üéµ Target: ${sampleRate}Hz, ${channels} channels`)

  if (!text || text.trim().length === 0) {
    console.error("[TTS] ‚ùå Empty text provided")
    return
  }

  try {
    // Choose appropriate format based on sample rate
    let outputFormat, elevenlabsRate
    if (sampleRate <= 8000) {
      outputFormat = "pcm_16000"
      elevenlabsRate = 16000
    } else if (sampleRate <= 22050) {
      outputFormat = "pcm_22050"
      elevenlabsRate = 22050
    } else {
      outputFormat = "pcm_44100"
      elevenlabsRate = 44100
    }
    
    console.log(`[TTS] üéß Using ElevenLabs format: ${outputFormat} (${elevenlabsRate}Hz)`)
    
    const requestBody = {
      text: text.trim(),
      output_format: outputFormat,
      voice_settings: {
        stability: 0.7,
        similarity_boost: 0.8,
        style: 0.2,
        use_speaker_boost: true
      }
    }
    
    console.log("[TTS] üì§ Request body:", JSON.stringify(requestBody, null, 2))
    
    const resp = await fetch(
      `https://api.elevenlabs.io/v1/text-to-speech/${ELEVEN_VOICE_ID}`,
      {
        method: "POST",
        headers: {
          "xi-api-key": ELEVEN_API_KEY,
          Accept: "audio/pcm",
          "Content-Type": "application/json",
        },
        body: JSON.stringify(requestBody),
      }
    )
    
    console.log(`[TTS] üì• Response status: ${resp.status}`)
    console.log(`[TTS] üìã Response headers:`, Object.fromEntries(resp.headers))
    
    if (!resp.ok) {
      const errorText = await resp.text()
      throw new Error(`ElevenLabs API ${resp.status}: ${errorText}`)
    }

    let audioBuf = Buffer.from(await resp.arrayBuffer())
    console.log(`[TTS] üì¶ ElevenLabs returned ${audioBuf.length} bytes`)
    
    if (audioBuf.length === 0) {
      console.error("[TTS] ‚ùå Empty audio buffer from ElevenLabs")
      return
    }

    logAudioSample("TTS-ORIGINAL", audioBuf, 32)

    // Validate audio quality
    const maxSample = Math.max(...Array.from({length: Math.min(100, audioBuf.length/2)}, (_, i) => Math.abs(audioBuf.readInt16LE(i * 2))))
    console.log(`[TTS] üîä Max sample value: ${maxSample} (should be > 1000 for good audio)`)

    if (maxSample < 100) {
      console.warn("[TTS] ‚ö†Ô∏è Audio seems very quiet or silent")
    }

    // Save original for debugging
    try {
      const filename = `/tmp/debug_tts_original_${elevenlabsRate}hz_${Date.now()}.pcm`
      fs.writeFileSync(filename, audioBuf)
      console.log(`[TTS] üíæ Saved original to ${filename}`)
    } catch (err) {
      console.log(`[TTS] ‚ö†Ô∏è Could not save debug file: ${err.message}`)
    }

    // Resample if needed
    if (sampleRate !== elevenlabsRate) {
      console.log(`[TTS] üîÑ Resampling from ${elevenlabsRate}Hz to ${sampleRate}Hz`)
      audioBuf = resampleAudio(audioBuf, elevenlabsRate, sampleRate)
      
      try {
        const filename = `/tmp/debug_tts_resampled_${sampleRate}hz_${Date.now()}.pcm`
        fs.writeFileSync(filename, audioBuf)
        console.log(`[TTS] üíæ Saved resampled to ${filename}`)
      } catch (err) {
        console.log(`[TTS] ‚ö†Ô∏è Could not save resampled file: ${err.message}`)
      }
    }

    // Validate final buffer before streaming
    const finalBase64 = audioBuf.toString('base64')
    if (!validateBase64Audio(finalBase64, "TTS-FINAL")) {
      console.error("[TTS] ‚ùå Final audio buffer failed base64 validation")
      return
    }

    console.log(`[TTS] üöÄ Starting audio stream...`)
    
    const sentChunks = await streamAudioToCallRealtime({
      ws,
      streamId,
      channelId,
      callId,
      pcmBuffer: audioBuf,
      sampleRate,
      channels,
      callDirection,
    })

    console.log(`[TTS] ‚úÖ Successfully sent ${sentChunks} audio chunks`)
    
    if (sentChunks === 0) {
      console.error("[TTS] ‚ùå No chunks were sent!")
    }
    
  } catch (err) {
    console.error(`[TTS] ‚ùå ElevenLabs error: ${err.message}`)
    console.error(`[TTS] Stack trace:`, err.stack)
  }
}

// -------- Sarvam HTTP TTS (8000 Hz, PBX format) --------
async function synthesizeAndStreamAudioSarvam({ text, ws, streamId, channelId, callId, sampleRate = 8000, channels = 1, callDirection, language = "en", speaker = "pavithra" }) {
  console.log(`\n[TTS-START:SARVAM] ${ts()}`)
  console.log(`  üìù Text: "${text}"`)
  console.log(`  üéµ Target: ${sampleRate}Hz, ${channels} channels (forced 8000Hz)`) 

  if (!text || text.trim().length === 0) {
    console.error("[TTS:SARVAM] ‚ùå Empty text provided")
    return
  }

  try {
    const body = {
      inputs: [text.trim()],
      target_language_code: language,
      speaker,
      pitch: 0,
      pace: 1.0,
      loudness: 1.0,
      speech_sample_rate: 8000,
      enable_preprocessing: true,
      model: "bulbul:v1",
    }

    console.log("[TTS:SARVAM] üì§ Request body:", JSON.stringify(body))

    const resp = await fetch("https://api.sarvam.ai/text-to-speech", {
      method: "POST",
      headers: {
        "Content-Type": "application/json",
        "API-Subscription-Key": SARVAM_API_KEY,
      },
      body: JSON.stringify(body),
    })

    console.log(`[TTS:SARVAM] üì• Response status: ${resp.status}`)

    if (!resp.ok) {
      const errorText = await resp.text()
      throw new Error(`Sarvam API ${resp.status}: ${errorText}`)
    }

    const json = await resp.json()
    const audioBase64 = json?.audios?.[0]
    if (!audioBase64) {
      throw new Error("Sarvam response missing audio data")
    }

    const audioBuf = Buffer.from(audioBase64, "base64")
    console.log(`[TTS:SARVAM] üì¶ Received ${audioBuf.length} bytes @ 8000Hz`)
    logAudioSample("TTS-SARVAM", audioBuf, 32)

    // Stream out in exact PBX media event format
    const sentChunks = await streamAudioToCallRealtime({
      ws,
      streamId,
      channelId,
      callId,
      pcmBuffer: audioBuf,
      sampleRate: 8000,
      channels: 1,
      callDirection,
    })

    console.log(`[TTS:SARVAM] ‚úÖ Sent ${sentChunks} audio chunks`)
  } catch (err) {
    console.error(`[TTS:SARVAM] ‚ùå Error: ${err.message}`)
  }
}

// -------- OpenAI GPT --------
async function getAiResponse(prompt) {
  try {
    console.log(`[AI] ü§ñ Processing prompt: "${prompt}"`)
    
    const resp = await fetch("https://api.openai.com/v1/chat/completions", {
      method: "POST",
      headers: {
        Authorization: `Bearer ${OPENAI_API_KEY}`,
        "Content-Type": "application/json",
      },
      body: JSON.stringify({
        model: "gpt-4o-mini",
        messages: [
          { 
            role: "system", 
            content: "You are a helpful voice assistant for phone calls. Keep responses short and natural, under 25 words. Be friendly and conversational."
          },
          { role: "user", content: prompt },
        ],
        max_tokens: 60,
        temperature: 0.7
      }),
    })
    
    if (!resp.ok) {
      const errorText = await resp.text()
      throw new Error(`OpenAI ${resp.status}: ${errorText}`)
    }
    
    const data = await resp.json()
    const response = data.choices[0].message.content.trim()
    console.log(`[AI] üí¨ Generated response: "${response}"`)
    return response
    
  } catch (err) {
    console.error(`[AI] ‚ùå Error: ${err.message}`)
    return "I understand. How can I help you?"
  }
}

// -------- MAIN SanPBX Handler --------
function setupSanPbxWebSocketServer(ws) {
  console.log("\nüöÄ Setting up SanIPPBX voice server connection")

  let streamId = null
  let channelId = null
  let callId = null
  let callDirection = "Incoming"
  let mediaFormat = { encoding: "LINEAR16", sampleRate: 8000, channels: 1 }
  let dgWs = null
  let callAnswered = false
  let isProcessing = false
  let incomingMediaCount = 0
  let hasReceivedAudio = false

  const connectDeepgram = (sampleRate, encoding) => {
    const dgEncoding = 'linear16'
    console.log(`\n[STT-SETUP] üé§ Connecting to Deepgram`)
    console.log(`  Encoding: ${dgEncoding} (from PBX: ${encoding})`)
    console.log(`  Sample rate: ${sampleRate}Hz`)
    
    if (dgWs) {
      try { dgWs.close() } catch {}
    }
    
    dgWs = new DGWebSocket(
      `wss://api.deepgram.com/v1/listen?encoding=${dgEncoding}&sample_rate=${sampleRate}&channels=1&model=nova-2&interim_results=false&smart_format=true&endpointing=300&utterance_end_ms=1000&punctuate=true`,
      { headers: { Authorization: `Token ${DEEPGRAM_API_KEY}` } }
    )
    
    dgWs.on("open", () => {
      console.log("[STT] ‚úÖ Connected to Deepgram successfully")
    })
    
    dgWs.on("message", async (msg) => {
      try {
        const data = JSON.parse(msg.toString())
        
        // Only log non-empty responses
        if (data.channel?.alternatives?.[0]?.transcript) {
          console.log("[STT] üì• Deepgram response:", JSON.stringify(data, null, 2))
        }
        
        const transcript = data.channel?.alternatives?.[0]?.transcript?.trim()
        
        if (transcript && transcript.length > 0 && !isProcessing) {
          console.log(`\nüé§ [STT] Transcript: "${transcript}"`)
          isProcessing = true
          
          try {
            const lowerTranscript = transcript.toLowerCase()
            
            if (lowerTranscript.includes('test') || lowerTranscript.includes('tone')) {
              console.log("[DEBUG] üîä Test audio command detected")
              const testBuffer = await testAudioGeneration(mediaFormat.sampleRate)
              await streamAudioToCallRealtime({
                ws,
                streamId,
                channelId,
                callId,
                pcmBuffer: testBuffer,
                sampleRate: mediaFormat.sampleRate,
                channels: mediaFormat.channels,
                callDirection,
              })
            } else {
              const reply = await getAiResponse(transcript)
              console.log(`ü§ñ [AI] Response: "${reply}"`)
              
              await synthesizeAndStreamAudioSarvam({
                text: reply,
                ws,
                streamId,
                channelId,
                callId,
                sampleRate: 8000,
                channels: 1,
                callDirection,
                language: "en",
                speaker: "pavithra",
              })
            }
          } finally {
            // Longer delay to prevent overlap
            setTimeout(() => {
              isProcessing = false
            }, 2000)
          }
        }
      } catch (error) {
        console.error("[STT] ‚ùå Error processing message:", error.message)
        isProcessing = false
      }
    })
    
    dgWs.on("close", (code, reason) => {
      console.log(`[STT] üîå Deepgram closed: ${code} - ${reason}`)
    })
    
    dgWs.on("error", (e) => {
      console.error("[STT] ‚ùå Deepgram error:", e.message)
    })
  }

  ws.on("message", async (raw) => {
    let data
    try {
      data = JSON.parse(raw.toString())
    } catch (parseError) {
      console.error("[SANPBX] ‚ùå Failed to parse message:", parseError.message)
      return
    }
    
    console.log(`\nüì® [SANPBX] Received event: ${data.event}`)
    
    switch (data.event) {
      case "connected":
        console.log("üîó [SANPBX] Connected event")
        logFullData("CONNECTED", data)
        
        // Extract call info from connected event
        streamId = data.streamId
        channelId = data.channelId
        callId = data.callId
        callDirection = data.callDirection || "Incoming"
        
        console.log(`[SANPBX] üÜî Call Info - StreamId: ${streamId}, ChannelId: ${channelId}, CallId: ${callId}`)
        break
        
      case "start":
        console.log("‚ñ∂Ô∏è [SANPBX] Call started")
        logFullData("START", data)
        
        // Update call info if provided
        if (data.streamId) streamId = data.streamId
        if (data.channelId) channelId = data.channelId
        if (data.callId) callId = data.callId
        if (data.callDirection) callDirection = data.callDirection
        
        if (data.mediaFormat) {
          mediaFormat = {
            encoding: data.mediaFormat.encoding || "LINEAR16",
            sampleRate: 8000,
            channels: 1
          }
        } else {
          mediaFormat = { encoding: "LINEAR16", sampleRate: 8000, channels: 1 }
        }
        
        console.log(`[SANPBX] üÜî Stream ID: ${streamId}`)
        console.log(`[SANPBX] üéµ Media Format:`, mediaFormat)
        
        connectDeepgram(mediaFormat.sampleRate, mediaFormat.encoding)
        break
        
      case "answer":
        console.log("üìû [SANPBX] Call answered")
        logFullData("ANSWER", data)
        
        callAnswered = true
        
        // Update call info
        if (data.streamId) streamId = data.streamId
        if (data.channelId) channelId = data.channelId
        if (data.callId) callId = data.callId
        if (data.callDirection) callDirection = data.callDirection
        
        console.log(`[SANPBX] üÜî Updated Call Info - StreamId: ${streamId}, ChannelId: ${channelId}, CallId: ${callId}`)
        
        // Send ACK immediately
        const ack = {
          event: "answer",
          callId: callId,
          channelId: channelId,
          streamId: streamId,
        }
        
        console.log("[SANPBX] üì§ Sending ACK:")
        logFullData("ACK", ack)
        
        try {
          ws.send(JSON.stringify(ack))
          console.log("[SANPBX] ‚úÖ Sent answer ACK")
        } catch (err) {
          console.error("[SANPBX] ‚ùå Failed to send ACK:", err.message)
        }

        // Start greeting sequence
        setTimeout(async () => {
          if (!isProcessing && callAnswered && streamId && channelId && callId) {
            isProcessing = true
            
            try {
              console.log("\nüîä [DEBUG] Starting audio sequence...")
              
              // Send a strong test tone first
              console.log("1Ô∏è‚É£ Sending test tone...")
              const testBuffer = await testAudioGeneration(mediaFormat.sampleRate, 3.0)
              const testSent = await streamAudioToCallRealtime({
                ws,
                streamId,
                channelId,
                callId,
                pcmBuffer: testBuffer,
                sampleRate: mediaFormat.sampleRate,
                channels: mediaFormat.channels,
                callDirection,
              })
              
              console.log(`[DEBUG] ‚úÖ Test tone: ${testSent} chunks sent`)
              
              // Wait then send greeting
              setTimeout(async () => {
                console.log("2Ô∏è‚É£ Sending greeting...")
                const greeting = "Hello! This is your AI assistant. I'm sending audio now. Can you hear me clearly?"
                await synthesizeAndStreamAudioSarvam({
                  text: greeting,
                  ws,
                  streamId,
                  channelId,
                  callId,
                  sampleRate: 8000,
                  channels: 1,
                  callDirection,
                  language: "en",
                  speaker: "pavithra",
                })
                
                setTimeout(() => {
                  isProcessing = false
                  console.log("‚úÖ [DEBUG] Audio sequence complete")
                }, 2000)
              }, 3000)
              
            } catch (err) {
              console.error("[DEBUG] ‚ùå Error in audio sequence:", err.message)
              isProcessing = false
            }
          }
        }, 2000)
        break

      case "media": {
        incomingMediaCount++
        
        // Updated to match the exact format: payload is directly in the event object
        const b64 = data?.payload  // Changed from data?.media?.payload to data?.payload
        
        if (incomingMediaCount <= 3) {
          console.log(`\nüéµ [MEDIA-${incomingMediaCount}] Received media packet`)
          console.log(`[MEDIA-${incomingMediaCount}] Chunk: ${data.chunk}, Duration: ${data.chunk_durn_ms}ms`)
          
          if (b64 && b64.length > 0) {
            const buffer = Buffer.from(b64, "base64")
            console.log(`[MEDIA-${incomingMediaCount}] ‚úÖ Payload: ${b64.length} chars -> ${buffer.length} bytes`)
            logAudioSample(`MEDIA-${incomingMediaCount}`, buffer, 16)
            
            if (!hasReceivedAudio) {
              hasReceivedAudio = true
              console.log("üéâ [MEDIA] First audio received!")
            }
          } else {
            console.log(`[MEDIA-${incomingMediaCount}] ‚ùå No payload`)
          }
        } else if (incomingMediaCount === 4) {
          console.log(`[MEDIA] ... (received ${incomingMediaCount}+ packets)`)
        }
        
        // Forward to Deepgram if we have audio
        if (b64 && b64.length > 0 && dgWs && dgWs.readyState === WebSocket.OPEN && callAnswered) {
          try {
            const audioBuffer = Buffer.from(b64, "base64")
            if (audioBuffer.length > 0) {
              dgWs.send(audioBuffer)
            }
          } catch (err) {
            console.error("[STT] ‚ùå Error forwarding to Deepgram:", err.message)
          }
        }
        break
      }
      
      case "dtmf":
        console.log(`üì± [SANPBX] DTMF: ${data.digit}`)
        
        if (!isProcessing) {
          isProcessing = true
          console.log(`[DEBUG] üîä DTMF ${data.digit} - immediate audio response`)
          
          setTimeout(async () => {
            try {
              // Send test tone
              const testBuffer = await testAudioGeneration(mediaFormat.sampleRate, 2.0)
              await streamAudioToCallRealtime({
                ws,
                streamId,
                channelId,
                callId,
                pcmBuffer: testBuffer,
                sampleRate: mediaFormat.sampleRate,
                channels: mediaFormat.channels,
                callDirection,
              })
              
              // Send voice response
              const dtmfResponse = `You pressed ${data.digit}. Audio test complete.`
              await synthesizeAndStreamAudioSarvam({
                text: dtmfResponse,
                ws,
                streamId,
                channelId,
                callId,
                sampleRate: 8000,
                channels: 1,
                callDirection,
                language: "en",
                speaker: "pavithra",
              })
            } catch (err) {
              console.error("[DEBUG] ‚ùå DTMF response error:", err.message)
            } finally {
              setTimeout(() => {
                isProcessing = false
              }, 1000)
            }
          }, 200)
        }
        break
        
      case "stop":
        console.log("üõë [SANPBX] Call stopped")
        
        callAnswered = false
        isProcessing = false
        incomingMediaCount = 0
        hasReceivedAudio = false
        streamId = null
        channelId = null
        callId = null
        
        if (dgWs) {
          try { dgWs.close() } catch {}
        }
        break
        
      case "transfer-call-response":
        console.log("üîÑ [SANPBX] Transfer call response")
        logFullData("TRANSFER-RESPONSE", data)
        break
        
      case "hangup-call-response":
        console.log("üìµ [SANPBX] Hangup call response")
        logFullData("HANGUP-RESPONSE", data)
        break
        
      default:
        console.log(`‚ùì [SANPBX] Unknown event: ${data.event}`)
        logFullData("UNKNOWN-EVENT", data)
        break
    }
  })
  
  ws.on("error", (error) => {
    console.error("[SANPBX] ‚ùå WebSocket error:", error.message)
  })
  
  ws.on("close", (code, reason) => {
    console.log(`[SANPBX] üîå WebSocket connection closed: ${code} - ${reason}`)
    callAnswered = false
    isProcessing = false
    incomingMediaCount = 0
    hasReceivedAudio = false
    streamId = null
    channelId = null
    callId = null
    
    if (dgWs) {
      try { dgWs.close() } catch {}
    }
  })
}

module.exports = { setupSanPbxWebSocketServer }