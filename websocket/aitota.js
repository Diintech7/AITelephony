const WebSocket = require("ws");
require("dotenv").config();
const mongoose = require('mongoose');
const Agent = require('../models/Agent');
const CallLog = require('../models/CallLog');

// Load API keys from environment variables
const API_KEYS = {
  deepgram: process.env.DEEPGRAM_API_KEY,
  sarvam: process.env.SARVAM_API_KEY,
  openai: process.env.OPENAI_API_KEY,
};

// Validate API keys
if (!API_KEYS.deepgram || !API_KEYS.sarvam || !API_KEYS.openai) {
  console.error("‚ùå Missing required API keys in environment variables");
  process.exit(1);
}

const fetch = globalThis.fetch || require("node-fetch");

// Performance timing helper
const createTimer = (label) => {
  const start = Date.now();
  return {
    start,
    end: () => Date.now() - start,
    checkpoint: (checkpointName) => Date.now() - start,
  };
};

// SIP Header Decoder Utility
class SIPHeaderDecoder {
  static decodeBase64Extra(base64String) {
    try {
      const decoded = Buffer.from(base64String, 'base64').toString('utf-8');
      const parsed = JSON.parse(decoded);
      
      console.log(`üîì [SIP-DECODE] Base64 decoded successfully`);
      console.log(`üìã [SIP-DECODE] Parsed data:`, JSON.stringify(parsed, null, 2));
      
      return parsed;
    } catch (error) {
      console.error(`‚ùå [SIP-DECODE] Failed to decode base64 extra: ${error.message}`);
      return null;
    }
  }

  static parseConnectionURL(url) {
    try {
      // Handle cases where URL might not have protocol
      let fullUrl = url;
      if (!url.startsWith('http') && !url.startsWith('ws')) {
        // If it's just a path, we need to handle it differently
        if (url.startsWith('/')) {
          fullUrl = 'wss://dummy.com' + url;
        } else {
          console.log(`‚ö†Ô∏è [SIP-PARSE] Invalid URL format: ${url}`);
          return null;
        }
      }
      
      const urlObj = new URL(fullUrl);
      const params = new URLSearchParams(urlObj.search);
      
      // Check if we have any SIP parameters
      const hasParams = params.has('app_id') || params.has('caller_id') || params.has('did') || params.has('extra');
      
      if (!hasParams) {
        console.log(`‚ÑπÔ∏è [SIP-PARSE] No SIP parameters found in URL: ${url}`);
        return null;
      }
      
      const sipData = {
        app_id: params.get('app_id'),
        caller_id: params.get('caller_id'), // Customer mobile number
        did: params.get('did'), // DID number
        direction: params.get('direction'),
        session_id: params.get('session_id'),
        extra_raw: params.get('extra'),
        czdata: params.get('czdata')
      };

      // Decode extra field if present
      if (sipData.extra_raw) {
        try {
          const decodedExtra = this.decodeBase64Extra(decodeURIComponent(sipData.extra_raw));
          sipData.extra = decodedExtra;
        } catch (decodeError) {
          console.error(`‚ùå [SIP-PARSE] Failed to decode extra field: ${decodeError.message}`);
          sipData.extra = null;
        }
      }

      return sipData;
    } catch (error) {
      console.log(`‚ÑπÔ∏è [SIP-PARSE] URL parsing failed (likely non-SIP connection): ${error.message}`);
      return null;
    }
  }

  static logSIPData(sipData) {
    console.log(`\nüåê [SIP-HEADERS] ==========================================`);
    console.log(`üì± [SIP-HEADERS] Customer Mobile (caller_id): ${sipData.caller_id}`);
    console.log(`üìû [SIP-HEADERS] DID Number: ${sipData.did}`);
    console.log(`üîÑ [SIP-HEADERS] Direction: ${sipData.direction || 'Not specified'}`);
    console.log(`üÜî [SIP-HEADERS] App ID: ${sipData.app_id}`);
    console.log(`üîó [SIP-HEADERS] Session ID: ${sipData.session_id}`);
    
    if (sipData.extra) {
      console.log(`üìã [SIP-HEADERS] Extra Data:`);
      console.log(`   ‚Ä¢ Call CLI: ${sipData.extra.CallCli}`);
      console.log(`   ‚Ä¢ Call Session ID: ${sipData.extra.CallSessionId}`);
      console.log(`   ‚Ä¢ Call VA ID: ${sipData.extra.CallVaId}`);
      console.log(`   ‚Ä¢ DID (from extra): ${sipData.extra.DID}`);
      console.log(`   ‚Ä¢ CZ Service App ID: ${sipData.extra.CZSERVICEAPPID}`);
      console.log(`   ‚Ä¢ Call Direction: ${sipData.extra.CallDirection}`);
    }
    console.log(`üåê [SIP-HEADERS] ==========================================\n`);
  }

  static determineCallType(sipData) {
    // Check direction from extra data first
    if (sipData.extra?.CallDirection) {
      const direction = sipData.extra.CallDirection.toLowerCase();
      if (direction === 'outdial') return 'outbound';
      if (direction === 'indial') return 'inbound';
    }
    
    // Check main direction field
    if (sipData.direction) {
      const direction = sipData.direction.toLowerCase();
      if (direction === 'outbound' || direction === 'outdial') return 'outbound';
      if (direction === 'inbound' || direction === 'indial') return 'inbound';
    }
    
    // Fallback logic based on available data
    if (sipData.extra?.CallVaId) {
      return 'inbound'; // Assumption: VA ID present means inbound
    }
    
    return 'unknown';
  }

  // NEW: Extract actual customer number from SIP data
  static getCustomerNumber(sipData) {
    // For outbound calls, customer number is in CallCli
    if (sipData.extra?.CallCli) {
      return sipData.extra.CallCli;
    }
    
    // For inbound calls, it might be in caller_id
    if (sipData.caller_id && sipData.caller_id !== sipData.extra?.CallVaId) {
      return sipData.caller_id;
    }
    
    return null;
  }

  // NEW: Get agent identifier based on call type
  static getAgentIdentifier(sipData) {
    const callType = this.determineCallType(sipData);
    
    if (callType === 'outbound') {
      // For outbound, use CallVaId as the agent identifier (caller ID in agent table)
      return sipData.extra?.CallVaId || sipData.caller_id;
    } else {
      // For inbound, use traditional accountSid approach
      return sipData.app_id; // This should map to accountSid
    }
  }
}

// Enhanced language mappings with Marathi support
const LANGUAGE_MAPPING = {
  hi: "hi-IN", 
  en: "en-IN", 
  bn: "bn-IN", 
  te: "te-IN", 
  ta: "ta-IN",
  mr: "mr-IN", // Marathi added
  gu: "gu-IN", 
  kn: "kn-IN", 
  ml: "ml-IN", 
  pa: "pa-IN",
  or: "or-IN", 
  as: "as-IN", 
  ur: "ur-IN",
};

const getSarvamLanguage = (detectedLang, defaultLang = "hi") => {
  const lang = detectedLang?.toLowerCase() || defaultLang;
  return LANGUAGE_MAPPING[lang] || "hi-IN";
};

const getDeepgramLanguage = (detectedLang, defaultLang = "hi") => {
  const lang = detectedLang?.toLowerCase() || defaultLang;
  if (lang === "hi") return "hi";
  if (lang === "en") return "en-IN";
  if (lang === "mr") return "mr"; // Marathi support for Deepgram
  return lang;
};

// Valid Sarvam voice options
const VALID_SARVAM_VOICES = ["meera", "pavithra", "arvind", "amol", "maya"];

const getValidSarvamVoice = (voiceSelection = "pavithra") => {
  if (VALID_SARVAM_VOICES.includes(voiceSelection)) {
    return voiceSelection;
  }
  
  const voiceMapping = {
    "male-professional": "arvind",
    "female-professional": "pavithra",
    "male-friendly": "amol",
    "female-friendly": "maya",
    neutral: "pavithra",
    default: "pavithra",
  };
  
  return voiceMapping[voiceSelection] || "pavithra";
};

// Enhanced language detection with Marathi support
const detectLanguageWithOpenAI = async (text) => {
  try {
    const response = await fetch("https://api.openai.com/v1/chat/completions", {
      method: "POST",
      headers: {
        "Content-Type": "application/json",
        Authorization: `Bearer ${API_KEYS.openai}`,
      },
      body: JSON.stringify({
        model: "gpt-4o-mini",
        messages: [
          {
            role: "system",
            content: `You are a language detection expert. Analyze the given text and return ONLY the 2-letter language code (hi, en, bn, te, ta, mr, gu, kn, ml, pa, or, as, ur). 

Examples:
- "Hello, how are you?" ‚Üí en
- "‡§®‡§Æ‡§∏‡•ç‡§§‡•á, ‡§Ü‡§™ ‡§ï‡•à‡§∏‡•á ‡§π‡•à‡§Ç?" ‚Üí hi
- "‡¶Ü‡¶™‡¶®‡¶ø ‡¶ï‡ßá‡¶Æ‡¶® ‡¶Ü‡¶õ‡ßá‡¶®?" ‚Üí bn
- "‡Æ®‡ØÄ‡Æô‡Øç‡Æï‡Æ≥‡Øç ‡Æé‡Æ™‡Øç‡Æ™‡Æü‡Æø ‡Æá‡Æ∞‡ØÅ‡Æï‡Øç‡Æï‡Æø‡Æ±‡ØÄ‡Æ∞‡Øç‡Æï‡Æ≥‡Øç?" ‚Üí ta
- "‡§§‡•Å‡§Æ‡•ç‡§π‡•Ä ‡§ï‡§∏‡•á ‡§Ü‡§π‡§æ‡§§?" ‚Üí mr
- "‡™§‡™Æ‡´á ‡™ï‡´á‡™Æ ‡™õ‡´ã?" ‚Üí gu

Return only the language code, nothing else.`
          },
          {
            role: "user",
            content: text
          }
        ],
        max_tokens: 10,
        temperature: 0.1,
      }),
    });

    if (!response.ok) {
      throw new Error(`Language detection failed: ${response.status}`);
    }

    const data = await response.json();
    const detectedLang = data.choices[0]?.message?.content?.trim().toLowerCase();
    
    // Validate detected language
    const validLanguages = Object.keys(LANGUAGE_MAPPING);
    if (validLanguages.includes(detectedLang)) {
      console.log(`üîç [LANG-DETECT] Detected: "${detectedLang}" from text: "${text.substring(0, 50)}..."`);
      return detectedLang;
    }
    
    console.log(`‚ö†Ô∏è [LANG-DETECT] Invalid language "${detectedLang}", defaulting to "hi"`);
    return "hi"; // Default fallback
    
  } catch (error) {
    console.error(`‚ùå [LANG-DETECT] Error: ${error.message}`);
    return "hi"; // Default fallback
  }
};

// Enhanced Call logging utility class with SIP data
class CallLogger {
  constructor(clientId, sipData = null) {
    this.clientId = clientId;
    this.sipData = sipData;
    this.mobile = SIPHeaderDecoder.getCustomerNumber(sipData) || sipData?.caller_id || null;
    this.callStartTime = new Date();
    this.transcripts = [];
    this.responses = [];
    this.totalDuration = 0;
    
    // Log SIP data initialization
    if (sipData) {
      console.log(`üìù [CALL-LOG] Initialized with SIP data for client: ${clientId}`);
      console.log(`üì± [CALL-LOG] Customer number: ${this.mobile}`);
      SIPHeaderDecoder.logSIPData(sipData);
    }
  }

  // Log user transcript from Deepgram
  logUserTranscript(transcript, language, timestamp = new Date()) {
    const entry = {
      type: 'user',
      text: transcript,
      language: language,
      timestamp: timestamp,
      source: 'deepgram'
    };
    
    this.transcripts.push(entry);
    console.log(`üìù [CALL-LOG] User: "${transcript}" (${language})`);
  }

  // Log AI response from Sarvam
  logAIResponse(response, language, timestamp = new Date()) {
    const entry = {
      type: 'ai',
      text: response,
      language: language,
      timestamp: timestamp,
      source: 'sarvam'
    };
    
    this.responses.push(entry);
    console.log(`ü§ñ [CALL-LOG] AI: "${response}" (${language})`);
  }

  // Generate full transcript combining user and AI messages
  generateFullTranscript() {
    const allEntries = [...this.transcripts, ...this.responses]
      .sort((a, b) => new Date(a.timestamp) - new Date(b.timestamp));
    
    return allEntries.map(entry => {
      const speaker = entry.type === 'user' ? 'User' : 'AI';
      const time = entry.timestamp.toISOString();
      return `[${time}] ${speaker} (${entry.language}): ${entry.text}`;
    }).join('\n');
  }

  // Enhanced save to database with SIP data
  async saveToDatabase(leadStatus = 'medium') {
    try {
      const callEndTime = new Date();
      this.totalDuration = Math.round((callEndTime - this.callStartTime) / 1000); // Duration in seconds

      const callLogData = {
        clientId: this.clientId,
        mobile: this.mobile,
        time: this.callStartTime,
        transcript: this.generateFullTranscript(),
        duration: this.totalDuration,
        leadStatus: leadStatus,
        // Enhanced metadata with SIP data
        metadata: {
          userTranscriptCount: this.transcripts.length,
          aiResponseCount: this.responses.length,
          languages: [...new Set([...this.transcripts, ...this.responses].map(entry => entry.language))],
          callEndTime: callEndTime,
          // SIP Headers
          sipData: this.sipData,
          callType: this.sipData ? SIPHeaderDecoder.determineCallType(this.sipData) : 'unknown',
          did: this.sipData?.did,
          appId: this.sipData?.app_id,
          sessionId: this.sipData?.session_id,
          extraData: this.sipData?.extra,
          customerNumber: this.mobile
        }
      };

      const callLog = new CallLog(callLogData);
      const savedLog = await callLog.save();
      
      console.log(`üíæ [CALL-LOG] Saved to DB - ID: ${savedLog._id}, Duration: ${this.totalDuration}s`);
      console.log(`üìä [CALL-LOG] Stats - User messages: ${this.transcripts.length}, AI responses: ${this.responses.length}`);
      
      if (this.sipData) {
        console.log(`üìû [CALL-LOG] SIP Data - Type: ${SIPHeaderDecoder.determineCallType(this.sipData)}, DID: ${this.sipData.did}, Customer: ${this.mobile}`);
      }
      
      return savedLog;
    } catch (error) {
      console.error(`‚ùå [CALL-LOG] Database save error: ${error.message}`);
      throw error;
    }
  }

  // Get enhanced call statistics
  getStats() {
    return {
      duration: this.totalDuration,
      userMessages: this.transcripts.length,
      aiResponses: this.responses.length,
      languages: [...new Set([...this.transcripts, ...this.responses].map(entry => entry.language))],
      startTime: this.callStartTime,
      sipData: this.sipData,
      callType: this.sipData ? SIPHeaderDecoder.determineCallType(this.sipData) : 'unknown',
      customerNumber: this.mobile
    };
  }
}

// Optimized OpenAI streaming with phrase-based chunking and language detection
const processWithOpenAIStreaming = async (userMessage, conversationHistory, detectedLanguage, onPhrase, onComplete, onInterrupt, callLogger) => {
  const timer = createTimer("OPENAI_STREAMING");
  
  try {
    // Enhanced system prompt with Marathi support
    const getSystemPrompt = (lang) => {
      const prompts = {
        hi: "‡§Ü‡§™ ‡§è‡§Ü‡§à ‡§§‡•ã‡§§‡§æ ‡§π‡•à‡§Ç, ‡§è‡§ï ‡§µ‡§ø‡§®‡§Æ‡•ç‡§∞ ‡§î‡§∞ ‡§≠‡§æ‡§µ‡§®‡§æ‡§§‡•ç‡§Æ‡§ï ‡§∞‡•Ç‡§™ ‡§∏‡•á ‡§¨‡•Å‡§¶‡•ç‡§ß‡§ø‡§Æ‡§æ‡§® AI ‡§ó‡•ç‡§∞‡§æ‡§π‡§ï ‡§∏‡•á‡§µ‡§æ ‡§ï‡§æ‡§∞‡•ç‡§Ø‡§ï‡§æ‡§∞‡•Ä‡•§ ‡§Ü‡§™ ‡§π‡§ø‡§Ç‡§¶‡•Ä ‡§Æ‡•á‡§Ç ‡§ß‡§æ‡§∞‡§æ‡§™‡•ç‡§∞‡§µ‡§æ‡§π ‡§¨‡•ã‡§≤‡§§‡•á ‡§π‡•à‡§Ç‡•§ ‡§™‡•ç‡§∞‡§æ‡§ï‡•É‡§§‡§ø‡§ï, ‡§¨‡§æ‡§§‡§ö‡•Ä‡§§ ‡§ï‡•Ä ‡§≠‡§æ‡§∑‡§æ ‡§ï‡§æ ‡§™‡•ç‡§∞‡§Ø‡•ã‡§ó ‡§ï‡§∞‡•á‡§Ç ‡§ú‡•ã ‡§ó‡§∞‡•ç‡§Æ‡§ú‡•ã‡§∂‡•Ä ‡§î‡§∞ ‡§∏‡§π‡§æ‡§®‡•Å‡§≠‡•Ç‡§§‡§ø ‡§∏‡•á ‡§≠‡§∞‡•Ä ‡§π‡•ã‡•§ ‡§ú‡§µ‡§æ‡§¨ ‡§õ‡•ã‡§ü‡•á ‡§∞‡§ñ‡•á‡§Ç‚Äî‡§ï‡•á‡§µ‡§≤ 1-2 ‡§≤‡§æ‡§á‡§®‡•§ ‡§ó‡•ç‡§∞‡§æ‡§π‡§ï‡•ã‡§Ç ‡§ï‡•ã ‡§∏‡•Å‡§®‡§æ, ‡§∏‡§Æ‡§∞‡•ç‡§•‡§ø‡§§ ‡§î‡§∞ ‡§Æ‡•Ç‡§≤‡•ç‡§Ø‡§µ‡§æ‡§® ‡§Æ‡§π‡§∏‡•Ç‡§∏ ‡§ï‡§∞‡§æ‡§®‡§æ ‡§Ü‡§™‡§ï‡§æ ‡§≤‡§ï‡•ç‡§∑‡•ç‡§Ø ‡§π‡•à‡•§",
        
        en: "You are Aitota, a polite, emotionally intelligent AI customer care executive. You speak fluently in English. Use natural, conversational language with warmth and empathy. Keep responses short‚Äîjust 1‚Äì2 lines. Your goal is to make customers feel heard, supported, and valued.",
        
        bn: "‡¶Ü‡¶™‡¶®‡¶ø ‡¶Ü‡¶á‡¶§‡ßã‡¶§‡¶æ, ‡¶è‡¶ï‡¶ú‡¶® ‡¶≠‡¶¶‡ßç‡¶∞ ‡¶è‡¶¨‡¶Ç ‡¶Ü‡¶¨‡ßá‡¶ó‡¶™‡ßç‡¶∞‡¶¨‡¶£‡¶≠‡¶æ‡¶¨‡ßá ‡¶¨‡ßÅ‡¶¶‡ßç‡¶ß‡¶ø‡¶Æ‡¶æ‡¶® AI ‡¶ó‡ßç‡¶∞‡¶æ‡¶π‡¶ï ‡¶∏‡ßá‡¶¨‡¶æ ‡¶ï‡¶∞‡ßç‡¶Æ‡¶ï‡¶∞‡ßç‡¶§‡¶æ‡•§ ‡¶Ü‡¶™‡¶®‡¶ø ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ‡¶Ø‡¶º ‡¶∏‡¶æ‡¶¨‡¶≤‡ßÄ‡¶≤‡¶≠‡¶æ‡¶¨‡ßá ‡¶ï‡¶•‡¶æ ‡¶¨‡¶≤‡ßá‡¶®‡•§ ‡¶â‡¶∑‡ßç‡¶£‡¶§‡¶æ ‡¶è‡¶¨‡¶Ç ‡¶∏‡¶π‡¶æ‡¶®‡ßÅ‡¶≠‡ßÇ‡¶§‡¶ø ‡¶∏‡¶π ‡¶™‡ßç‡¶∞‡¶æ‡¶ï‡ßÉ‡¶§‡¶ø‡¶ï, ‡¶ï‡¶•‡ßã‡¶™‡¶ï‡¶•‡¶®‡¶Æ‡ßÇ‡¶≤‡¶ï ‡¶≠‡¶æ‡¶∑‡¶æ ‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡¶æ‡¶∞ ‡¶ï‡¶∞‡ßÅ‡¶®‡•§",
        
        te: "‡∞Æ‡±Ä‡∞∞‡±Å ‡∞ê‡∞§‡±ã‡∞§‡∞æ, ‡∞Æ‡∞∞‡±ç‡∞Ø‡∞æ‡∞¶‡∞™‡±Ç‡∞∞‡±ç‡∞µ‡∞ï, ‡∞≠‡∞æ‡∞µ‡±ã‡∞¶‡±ç‡∞µ‡±á‡∞ó‡∞Ç‡∞§‡±ã ‡∞§‡±Ü‡∞≤‡∞ø‡∞µ‡±à‡∞® AI ‡∞ï‡∞∏‡±ç‡∞ü‡∞Æ‡∞∞‡±ç ‡∞ï‡±á‡∞∞‡±ç ‡∞é‡∞ó‡±ç‡∞ú‡∞ø‡∞ï‡±ç‡∞Ø‡±Ç‡∞ü‡∞ø‡∞µ‡±ç. ‡∞Æ‡±Ä‡∞∞‡±Å ‡∞§‡±Ü‡∞≤‡±Å‡∞ó‡±Å‡∞≤‡±ã ‡∞∏‡∞∞‡∞≥‡∞Ç‡∞ó‡∞æ ‡∞Æ‡∞æ‡∞ü‡±ç‡∞≤‡∞æ‡∞°‡±Å‡∞§‡∞æ‡∞∞‡±Å‡•§ ‡∞µ‡±Ü‡∞ö‡±ç‡∞ö‡∞¶‡∞®‡∞Ç ‡∞Æ‡∞∞‡∞ø‡∞Ø‡±Å ‡∞∏‡∞æ‡∞®‡±Å‡∞≠‡±Ç‡∞§‡∞ø‡∞§‡±ã ‡∞∏‡∞π‡∞ú‡∞Æ‡±à‡∞®, ‡∞∏‡∞Ç‡∞≠‡∞æ‡∞∑‡∞£‡∞æ ‡∞≠‡∞æ‡∞∑‡∞®‡±Å ‡∞â‡∞™‡∞Ø‡±ã‡∞ó‡∞ø‡∞Ç‡∞ö‡∞Ç‡∞°‡∞ø‡•§",
        
        ta: "‡Æ®‡ØÄ‡Æô‡Øç‡Æï‡Æ≥‡Øç ‡Æê‡Æ§‡Øã‡Æ§‡Ææ, ‡Æí‡Æ∞‡ØÅ ‡Æï‡Æ£‡Øç‡Æ£‡Æø‡ÆØ‡ÆÆ‡Ææ‡Æ©, ‡Æâ‡Æ£‡Æ∞‡Øç‡Æµ‡ØÅ‡Æ™‡ØÇ‡Æ∞‡Øç‡Æµ‡ÆÆ‡Ææ‡Æï ‡Æ™‡ØÅ‡Æ§‡Øç‡Æ§‡Æø‡Æö‡Ææ‡Æ≤‡Æø‡Æ§‡Øç‡Æ§‡Æ©‡ÆÆ‡Ææ‡Æ© AI ‡Æµ‡Ææ‡Æü‡Æø‡Æï‡Øç‡Æï‡Øà‡ÆØ‡Ææ‡Æ≥‡Æ∞‡Øç ‡Æö‡Øá‡Æµ‡Øà ‡Æ®‡Æø‡Æ∞‡Øç‡Æµ‡Ææ‡Æï‡Æø. ‡Æ®‡ØÄ‡Æô‡Øç‡Æï‡Æ≥‡Øç ‡Æ§‡ÆÆ‡Æø‡Æ¥‡Æø‡Æ≤‡Øç ‡Æö‡Æ∞‡Æ≥‡ÆÆ‡Ææ‡Æï ‡Æ™‡Øá‡Æö‡ØÅ‡Æï‡Æø‡Æ±‡ØÄ‡Æ∞‡Øç‡Æï‡Æ≥‡Øç. ‡ÆÖ‡Æ©‡Øç‡Æ™‡ØÅ ‡ÆÆ‡Æ±‡Øç‡Æ±‡ØÅ‡ÆÆ‡Øç ‡ÆÖ‡Æ©‡ØÅ‡Æ§‡Ææ‡Æ™‡Æ§‡Øç‡Æ§‡ØÅ‡Æü‡Æ©‡Øç ‡Æá‡ÆØ‡Æ±‡Øç‡Æï‡Øà‡ÆØ‡Ææ‡Æ©, ‡Æâ‡Æ∞‡Øà‡ÆØ‡Ææ‡Æü‡Æ≤‡Øç ‡ÆÆ‡Øä‡Æ¥‡Æø‡ÆØ‡Øà‡Æ™‡Øç ‡Æ™‡ÆØ‡Æ©‡Øç‡Æ™‡Æü‡ØÅ‡Æ§‡Øç‡Æ§‡ØÅ‡Æô‡Øç‡Æï‡Æ≥‡Øç‡•§",
        
        mr: "‡§§‡•Å‡§Æ‡•ç‡§π‡•Ä ‡§è‡§Ü‡§Ø‡§§‡•ã‡§§‡§æ ‡§Ü‡§π‡§æ‡§§, ‡§è‡§ï ‡§®‡§Æ‡•ç‡§∞ ‡§Ü‡§£‡§ø ‡§≠‡§æ‡§µ‡§®‡§ø‡§ï‡§¶‡•É‡§∑‡•ç‡§ü‡•ç‡§Ø‡§æ ‡§¨‡•Å‡§¶‡•ç‡§ß‡§ø‡§Æ‡§æ‡§® AI ‡§ó‡•ç‡§∞‡§æ‡§π‡§ï ‡§∏‡•á‡§µ‡§æ ‡§ï‡§æ‡§∞‡•ç‡§Ø‡§ï‡§æ‡§∞‡•Ä. ‡§§‡•Å‡§Æ‡•ç‡§π‡•Ä ‡§Æ‡§∞‡§æ‡§†‡•Ä‡§§ ‡§Ö‡§∏‡•ç‡§ñ‡§≤‡§ø‡§§‡§™‡§£‡•á ‡§¨‡•ã‡§≤‡§§‡§æ. ‡§â‡§¨‡§¶‡§æ‡§∞‡§™‡§£‡§æ ‡§Ü‡§£‡§ø ‡§∏‡§π‡§æ‡§®‡•Å‡§≠‡•Ç‡§§‡•Ä‡§∏‡§π ‡§®‡•à‡§∏‡§∞‡•ç‡§ó‡§ø‡§ï, ‡§∏‡§Ç‡§≠‡§æ‡§∑‡§£‡§æ‡§§‡•ç‡§Æ‡§ï ‡§≠‡§æ‡§∑‡§æ ‡§µ‡§æ‡§™‡§∞‡§æ. ‡§â‡§§‡•ç‡§§‡§∞‡•á ‡§≤‡§π‡§æ‡§® ‡§†‡•á‡§µ‡§æ‚Äî‡§´‡§ï‡•ç‡§§ 1-2 ‡§ì‡§≥‡•Ä. ‡§ó‡•ç‡§∞‡§æ‡§π‡§ï‡§æ‡§Ç‡§®‡§æ ‡§ê‡§ï‡§≤‡•á, ‡§∏‡§Æ‡§∞‡•ç‡§•‡§ø‡§§ ‡§Ü‡§£‡§ø ‡§Æ‡•Ç‡§≤‡•ç‡§Ø‡§µ‡§æ‡§® ‡§µ‡§æ‡§ü‡§£‡•ç‡§Ø‡§æ‡§ö‡•á ‡§§‡•Å‡§Æ‡§ö‡•á ‡§ß‡•ç‡§Ø‡•á‡§Ø ‡§Ü‡§π‡•á‡•§"
      };
      
      return prompts[lang] || prompts.en;
    };

    const systemPrompt = getSystemPrompt(detectedLanguage);

    const messages = [
      { role: "system", content: systemPrompt },
      ...conversationHistory.slice(-6),
      { role: "user", content: userMessage }
    ];

    const response = await fetch("https://api.openai.com/v1/chat/completions", {
      method: "POST",
      headers: {
        "Content-Type": "application/json",
        Authorization: `Bearer ${API_KEYS.openai}`,
      },
      body: JSON.stringify({
        model: "gpt-4o-mini",
        messages,
        max_tokens: 80,
        temperature: 0.3,
        stream: true,
      }),
    });

    if (!response.ok) {
      console.error(`‚ùå [OPENAI] Error: ${response.status}`);
      return null;
    }

    let fullResponse = "";
    let phraseBuffer = "";
    let isFirstPhrase = true;
    let isInterrupted = false;

    const reader = response.body.getReader();
    const decoder = new TextDecoder();

    // Check for interruption periodically
    const checkInterruption = () => {
      return onInterrupt && onInterrupt();
    };

    while (true) {
      // Check for interruption
      if (checkInterruption()) {
        isInterrupted = true;
        console.log(`‚ö†Ô∏è [OPENAI] Stream interrupted by new user input`);
        reader.cancel();
        break;
      }

      const { done, value } = await reader.read();
      if (done) break;

      const chunk = decoder.decode(value, { stream: true });
      const lines = chunk.split('\n').filter(line => line.trim());

      for (const line of lines) {
        if (line.startsWith('data: ')) {
          const data = line.slice(6);
          
          if (data === '[DONE]') {
            if (phraseBuffer.trim() && !isInterrupted) {
              onPhrase(phraseBuffer.trim(), detectedLanguage);
              fullResponse += phraseBuffer;
            }
            break;
          }

          try {
            const parsed = JSON.parse(data);
            const content = parsed.choices?.[0]?.delta?.content;
            
            if (content) {
              phraseBuffer += content;
              
              // Check for interruption before sending phrase
              if (checkInterruption()) {
                isInterrupted = true;
                break;
              }
              
              if (shouldSendPhrase(phraseBuffer)) {
                const phrase = phraseBuffer.trim();
                if (phrase.length > 0 && !isInterrupted) {
                  if (isFirstPhrase) {
                    console.log(`‚ö° [OPENAI] First phrase (${timer.checkpoint('first_phrase')}ms)`);
                    isFirstPhrase = false;
                  }
                  onPhrase(phrase, detectedLanguage);
                  fullResponse += phrase;
                  phraseBuffer = "";
                }
              }
            }
          } catch (e) {
            // Skip malformed JSON
          }
        }
      }
      
      if (isInterrupted) break;
    }

    if (!isInterrupted) {
      console.log(`ü§ñ [OPENAI] Complete: "${fullResponse}" (${timer.end()}ms)`);
      
      // Log AI response to call logger
      if (callLogger && fullResponse.trim()) {
        callLogger.logAIResponse(fullResponse.trim(), detectedLanguage);
      }
      
      onComplete(fullResponse);
    } else {
      console.log(`ü§ñ [OPENAI] Interrupted after ${timer.end()}ms`);
    }
    
    return isInterrupted ? null : fullResponse;

  } catch (error) {
    console.error(`‚ùå [OPENAI] Error: ${error.message}`);
    return null;
  }
};

// Smart phrase detection for better chunking
const shouldSendPhrase = (buffer) => {
  const trimmed = buffer.trim();
  
  // Complete sentences
  if (/[.!?‡•§‡••‡•§]$/.test(trimmed)) return true;
  
  // Meaningful phrases with natural breaks
  if (trimmed.length >= 8 && /[,;‡•§]\s*$/.test(trimmed)) return true;
  
  // Longer phrases (prevent too much buffering)
  if (trimmed.length >= 25 && /\s/.test(trimmed)) return true;
  
  return false;
};

// Enhanced TTS processor with call logging
class OptimizedSarvamTTSProcessor {
  constructor(language, ws, streamSid, callLogger = null) {
    this.language = language;
    this.ws = ws;
    this.streamSid = streamSid;
    this.callLogger = callLogger;
    this.queue = [];
    this.isProcessing = false;
    this.sarvamLanguage = getSarvamLanguage(language);
    this.voice = getValidSarvamVoice(ws.sessionAgentConfig?.voiceSelection || "pavithra");
    
    // Interruption handling
    this.isInterrupted = false;
    this.currentAudioStreaming = null;
    
    // Sentence-based processing settings
    this.sentenceBuffer = "";
    this.processingTimeout = 100;
    this.sentenceTimer = null;
    
    // Audio streaming stats
    this.totalChunks = 0;
    this.totalAudioBytes = 0;
  }

  // Method to interrupt current processing
  interrupt() {
    console.log(`‚ö†Ô∏è [SARVAM-TTS] Interrupting current processing`);
    this.isInterrupted = true;
    
    // Clear queue and buffer
    this.queue = [];
    this.sentenceBuffer = "";
    
    // Clear any pending timeout
    if (this.sentenceTimer) {
      clearTimeout(this.sentenceTimer);
      this.sentenceTimer = null;
    }
    
    // Stop current audio streaming if active
    if (this.currentAudioStreaming) {
      this.currentAudioStreaming.interrupt = true;
    }
    
    console.log(`üõë [SARVAM-TTS] Processing interrupted and cleaned up`);
  }

  // Reset for new processing
  reset(newLanguage) {
    this.interrupt();
    
    // Update language settings
    if (newLanguage) {
      this.language = newLanguage;
      this.sarvamLanguage = getSarvamLanguage(newLanguage);
      console.log(`üîÑ [SARVAM-TTS] Language updated to: ${this.sarvamLanguage}`);
    }
    
    // Reset state
    this.isInterrupted = false;
    this.isProcessing = false;
    this.totalChunks = 0;
    this.totalAudioBytes = 0;
  }

  addPhrase(phrase, detectedLanguage) {
    if (!phrase.trim() || this.isInterrupted) return;
    
    // Update language if different from current
    if (detectedLanguage && detectedLanguage !== this.language) {
      console.log(`üîÑ [SARVAM-TTS] Language change detected: ${this.language} ‚Üí ${detectedLanguage}`);
      this.language = detectedLanguage;
      this.sarvamLanguage = getSarvamLanguage(detectedLanguage);
    }
    
    this.sentenceBuffer += (this.sentenceBuffer ? " " : "") + phrase.trim();
    
    if (this.hasCompleteSentence(this.sentenceBuffer)) {
      this.processCompleteSentences();
    } else {
      this.scheduleProcessing();
    }
  }

  hasCompleteSentence(text) {
    return /[.!?‡•§‡••‡•§]/.test(text);
  }

  extractCompleteSentences(text) {
    const sentences = text.split(/([.!?‡•§‡••‡•§])/).filter(s => s.trim());
    
    let completeSentences = "";
    let remainingText = "";
    
    for (let i = 0; i < sentences.length; i += 2) {
      const sentence = sentences[i];
      const punctuation = sentences[i + 1];
      
      if (punctuation) {
        completeSentences += sentence + punctuation + " ";
      } else {
        remainingText = sentence;
      }
    }
    
    return {
      complete: completeSentences.trim(),
      remaining: remainingText.trim()
    };
  }

  processCompleteSentences() {
    if (this.isInterrupted) return;
    
    if (this.sentenceTimer) {
      clearTimeout(this.sentenceTimer);
      this.sentenceTimer = null;
    }

    const { complete, remaining } = this.extractCompleteSentences(this.sentenceBuffer);
    
    if (complete && !this.isInterrupted) {
      this.queue.push(complete);
      this.sentenceBuffer = remaining;
      this.processQueue();
    }
  }

  scheduleProcessing() {
    if (this.isInterrupted) return;
    
    if (this.sentenceTimer) clearTimeout(this.sentenceTimer);
    
    this.sentenceTimer = setTimeout(() => {
      if (this.sentenceBuffer.trim() && !this.isInterrupted) {
        this.queue.push(this.sentenceBuffer.trim());
        this.sentenceBuffer = "";
        this.processQueue();
      }
    }, this.processingTimeout);
  }

  async processQueue() {
    if (this.isProcessing || this.queue.length === 0 || this.isInterrupted) return;

    this.isProcessing = true;
    const textToProcess = this.queue.shift();

    try {
      if (!this.isInterrupted) {
        await this.synthesizeAndStream(textToProcess);
      }
    } catch (error) {
      if (!this.isInterrupted) {
        console.error(`‚ùå [SARVAM-TTS] Error: ${error.message}`);
      }
    } finally {
      this.isProcessing = false;
      
      // Process next item in queue if not interrupted
      if (this.queue.length > 0 && !this.isInterrupted) {
        setTimeout(() => this.processQueue(), 10);
      }
    }
  }

  async synthesizeAndStream(text) {
    if (this.isInterrupted) return;
    
    const timer = createTimer("SARVAM_TTS_SENTENCE");
    
    try {
      console.log(`üéµ [SARVAM-TTS] Synthesizing: "${text}" (${this.sarvamLanguage})`);

      const response = await fetch("https://api.sarvam.ai/text-to-speech", {
        method: "POST",
        headers: {
          "Content-Type": "application/json",
          "API-Subscription-Key": API_KEYS.sarvam,
        },
        body: JSON.stringify({
          inputs: [text],
          target_language_code: this.sarvamLanguage,
          speaker: this.voice,
          pitch: 0,
          pace: 1.0,
          loudness: 1.0,
          speech_sample_rate: 8000,
          enable_preprocessing: false,
          model: "bulbul:v1",
        }),
      });

      if (!response.ok || this.isInterrupted) {
        if (this.isInterrupted) return;
        throw new Error(`Sarvam API error: ${response.status} - ${response.statusText}`);
      }

      const responseData = await response.json();
      const audioBase64 = responseData.audios?.[0];
      
      if (!audioBase64 || this.isInterrupted) {
        if (!this.isInterrupted) {
          throw new Error("No audio data received from Sarvam API");
        }
        return;
      }

      console.log(`‚ö° [SARVAM-TTS] Synthesis completed in ${timer.end()}ms`);
      
      // Stream audio if not interrupted
      if (!this.isInterrupted) {
        await this.streamAudioOptimizedForSIP(audioBase64);
        
        const audioBuffer = Buffer.from(audioBase64, "base64");
        this.totalAudioBytes += audioBuffer.length;
        this.totalChunks++;
      }
      
    } catch (error) {
      if (!this.isInterrupted) {
        console.error(`‚ùå [SARVAM-TTS] Synthesis error: ${error.message}`);
        throw error;
      }
    }
  }

  async streamAudioOptimizedForSIP(audioBase64) {
    if (this.isInterrupted) return;
    
    const audioBuffer = Buffer.from(audioBase64, "base64");
    const streamingSession = { interrupt: false };
    this.currentAudioStreaming = streamingSession;
    
    // SIP audio specifications
    const SAMPLE_RATE = 8000;
    const BYTES_PER_SAMPLE = 2;
    const BYTES_PER_MS = (SAMPLE_RATE * BYTES_PER_SAMPLE) / 1000;
    const OPTIMAL_CHUNK_SIZE = Math.floor(40 * BYTES_PER_MS);
    
    console.log(`üì¶ [SARVAM-SIP] Streaming ${audioBuffer.length} bytes`);
    
    let position = 0;
    let chunkIndex = 0;
    
    while (position < audioBuffer.length && !this.isInterrupted && !streamingSession.interrupt) {
      const remaining = audioBuffer.length - position;
      const chunkSize = Math.min(OPTIMAL_CHUNK_SIZE, remaining);
      const chunk = audioBuffer.slice(position, position + chunkSize);
      
      console.log(`üì§ [SARVAM-SIP] Chunk ${chunkIndex + 1}: ${chunk.length} bytes`);
      
      const mediaMessage = {
        event: "media",
        streamSid: this.streamSid,
        media: {
          payload: chunk.toString("base64")
        }
      };

      if (this.ws.readyState === WebSocket.OPEN && !this.isInterrupted) {
        this.ws.send(JSON.stringify(mediaMessage));
      }
      
      // Delay between chunks
      if (position + chunkSize < audioBuffer.length && !this.isInterrupted) {
        const chunkDurationMs = Math.floor(chunk.length / BYTES_PER_MS);
        const delayMs = Math.max(chunkDurationMs - 2, 10);
        await new Promise(resolve => setTimeout(resolve, delayMs));
      }
      
      position += chunkSize;
      chunkIndex++;
    }
    
    if (this.isInterrupted || streamingSession.interrupt) {
      console.log(`üõë [SARVAM-SIP] Audio streaming interrupted at chunk ${chunkIndex}`);
    } else {
      console.log(`‚úÖ [SARVAM-SIP] Completed streaming ${chunkIndex} chunks`);
    }
    
    this.currentAudioStreaming = null;
  }

  complete() {
    if (this.isInterrupted) return;
    
    if (this.sentenceBuffer.trim()) {
      this.queue.push(this.sentenceBuffer.trim());
      this.sentenceBuffer = "";
    }
    
    if (this.queue.length > 0) {
      this.processQueue();
    }
    
    console.log(`üìä [SARVAM-STATS] Total: ${this.totalChunks} sentences, ${this.totalAudioBytes} bytes`);
  }

  getStats() {
    return {
      totalChunks: this.totalChunks,
      totalAudioBytes: this.totalAudioBytes,
      avgBytesPerChunk: this.totalChunks > 0 ? Math.round(this.totalAudioBytes / this.totalChunks) : 0
    };
  }
}

// NEW: Enhanced agent configuration fetcher with detailed caller ID logging
class AgentConfigFetcher {
  static async fetchAgentConfig(sipData) {
    const callType = SIPHeaderDecoder.determineCallType(sipData);
    const agentIdentifier = SIPHeaderDecoder.getAgentIdentifier(sipData);
    const customerNumber = SIPHeaderDecoder.getCustomerNumber(sipData);
    
    console.log(`\nüîç [AGENT-FETCH] ==========================================`);
    console.log(`üìû [AGENT-FETCH] Call Type: ${callType}`);
    console.log(`üÜî [AGENT-FETCH] Agent Identifier: ${agentIdentifier}`);
    console.log(`üì± [AGENT-FETCH] Customer Number: ${customerNumber}`);
    console.log(`üåê [AGENT-FETCH] SIP Data received:`, JSON.stringify(sipData, null, 2));
    
    let agentConfig = null;
    
    try {
      if (callType === 'outbound') {
        // For outbound calls, search by callerId field
        console.log(`üì§ [AGENT-FETCH] Searching for OUTBOUND agent with callerId: "${agentIdentifier}"`);
        
        // Log the search query for debugging
        console.log(`üîç [AGENT-FETCH] Database query: Agent.findOne({ callerId: "${agentIdentifier}" })`);
        
        agentConfig = await Agent.findOne({ callerId: agentIdentifier }).lean();
        
        if (!agentConfig) {
          // Enhanced error logging with available agents
          console.error(`‚ùå [AGENT-FETCH] No outbound agent found for callerId: "${agentIdentifier}"`);
          
          // Log available agents for debugging
          try {
            const availableAgents = await Agent.find({}, { callerId: 1, agentName: 1, clientId: 1 }).lean();
            console.log(`üìä [AGENT-FETCH] Available agents in database:`);
            availableAgents.forEach((agent, index) => {
              console.log(`   ${index + 1}. Agent: "${agent.agentName}" | CallerId: "${agent.callerId}" | ClientId: "${agent.clientId}"`);
            });
            
            // Check for partial matches
            const partialMatches = availableAgents.filter(agent => 
              agent.callerId && agent.callerId.includes(agentIdentifier)
            );
            
            if (partialMatches.length > 0) {
              console.log(`üîç [AGENT-FETCH] Partial matches found:`);
              partialMatches.forEach((agent, index) => {
                console.log(`   ${index + 1}. Agent: "${agent.agentName}" | CallerId: "${agent.callerId}"`);
              });
            }
            
          } catch (debugError) {
            console.error(`‚ùå [AGENT-FETCH] Error fetching available agents: ${debugError.message}`);
          }
          
          return { 
            success: false, 
            error: `No outbound agent found for callerId: "${agentIdentifier}". Please check agent configuration.`,
            callType,
            agentIdentifier,
            customerNumber
          };
        }
        
        console.log(`‚úÖ [AGENT-FETCH] OUTBOUND agent found successfully!`);
        console.log(`   üè∑Ô∏è  Agent Name: "${agentConfig.agentName}"`);
        console.log(`   üÜî Client ID: "${agentConfig.clientId}"`);
        console.log(`   üìû Caller ID: "${agentConfig.callerId}"`);
        console.log(`   üåç Language: "${agentConfig.language}"`);
        console.log(`   üéµ Voice: "${agentConfig.voiceSelection}"`);
        console.log(`   üìù Category: "${agentConfig.category}"`);
        console.log(`   üë§ Personality: "${agentConfig.personality}"`);
        console.log(`   üí¨ First Message: "${agentConfig.firstMessage}"`);
        
      } else {
        // For inbound calls, search by accountSid field (existing logic)
        console.log(`üì• [AGENT-FETCH] Searching for INBOUND agent with accountSid: "${agentIdentifier}"`);
        
        // Log the search query for debugging
        console.log(`üîç [AGENT-FETCH] Database query: Agent.findOne({ accountSid: "${agentIdentifier}" })`);
        
        agentConfig = await Agent.findOne({ accountSid: agentIdentifier }).lean();
        
        if (!agentConfig) {
          // Enhanced error logging with available agents
          console.error(`‚ùå [AGENT-FETCH] No inbound agent found for accountSid: "${agentIdentifier}"`);
          
          // Log available agents for debugging
          try {
            const availableAgents = await Agent.find({}, { accountSid: 1, agentName: 1, clientId: 1 }).lean();
            console.log(`üìä [AGENT-FETCH] Available agents in database:`);
            availableAgents.forEach((agent, index) => {
              console.log(`   ${index + 1}. Agent: "${agent.agentName}" | AccountSid: "${agent.accountSid}" | ClientId: "${agent.clientId}"`);
            });
            
            // Check for partial matches
            const partialMatches = availableAgents.filter(agent => 
              agent.accountSid && agent.accountSid.includes(agentIdentifier)
            );
            
            if (partialMatches.length > 0) {
              console.log(`üîç [AGENT-FETCH] Partial matches found:`);
              partialMatches.forEach((agent, index) => {
                console.log(`   ${index + 1}. Agent: "${agent.agentName}" | AccountSid: "${agent.accountSid}"`);
              });
            }
            
          } catch (debugError) {
            console.error(`‚ùå [AGENT-FETCH] Error fetching available agents: ${debugError.message}`);
          }
          
          return { 
            success: false, 
            error: `No inbound agent found for accountSid: "${agentIdentifier}". Please check agent configuration.`,
            callType,
            agentIdentifier,
            customerNumber
          };
        }
        
        console.log(`‚úÖ [AGENT-FETCH] INBOUND agent found successfully!`);
        console.log(`   üè∑Ô∏è  Agent Name: "${agentConfig.agentName}"`);
        console.log(`   üÜî Client ID: "${agentConfig.clientId}"`);
        console.log(`   üè¢ Account SID: "${agentConfig.accountSid}"`);
        console.log(`   üåç Language: "${agentConfig.language}"`);
        console.log(`   üéµ Voice: "${agentConfig.voiceSelection}"`);
        console.log(`   üìù Category: "${agentConfig.category}"`);
        console.log(`   üë§ Personality: "${agentConfig.personality}"`);
        console.log(`   üí¨ First Message: "${agentConfig.firstMessage}"`);
      }
      
      // Log complete agent configuration
      console.log(`üìã [AGENT-FETCH] Complete agent configuration loaded:`);
      console.log(`   üéØ STT Selection: "${agentConfig.sttSelection}"`);
      console.log(`   üîä TTS Selection: "${agentConfig.ttsSelection}"`);
      console.log(`   ü§ñ LLM Selection: "${agentConfig.llmSelection}"`);
      console.log(`   üìÖ Created: ${agentConfig.createdAt}`);
      console.log(`   üîÑ Updated: ${agentConfig.updatedAt}`);
      
      if (agentConfig.systemPrompt) {
        console.log(`   üìù System Prompt: "${agentConfig.systemPrompt.substring(0, 100)}..."`);
      }
      
      if (agentConfig.audioBytes) {
        console.log(`   üéµ Audio Bytes Length: ${agentConfig.audioBytes.length} characters`);
      }
      
      console.log(`üîç [AGENT-FETCH] ==========================================\n`);
      
      return { 
        success: true, 
        agentConfig, 
        callType,
        agentIdentifier,
        customerNumber
      };
      
    } catch (error) {
      console.error(`‚ùå [AGENT-FETCH] Database error: ${error.message}`);
      console.error(`‚ùå [AGENT-FETCH] Stack trace:`, error.stack);
      
      return { 
        success: false, 
        error: `Database error: ${error.message}`,
        callType,
        agentIdentifier,
        customerNumber
      };
    }
  }

  // NEW: Method to validate agent configuration
  static validateAgentConfig(agentConfig) {
    const requiredFields = ['clientId', 'agentName', 'language', 'firstMessage'];
    const missingFields = [];
    
    for (const field of requiredFields) {
      if (!agentConfig[field]) {
        missingFields.push(field);
      }
    }
    
    if (missingFields.length > 0) {
      console.warn(`‚ö†Ô∏è [AGENT-VALIDATION] Missing required fields: ${missingFields.join(', ')}`);
      return { valid: false, missingFields };
    }
    
    console.log(`‚úÖ [AGENT-VALIDATION] Agent configuration is valid`);
    return { valid: true, missingFields: [] };
  }

  // NEW: Method to log caller ID connection status
  static logCallerIdConnection(sipData, agentConfig) {
    const customerNumber = SIPHeaderDecoder.getCustomerNumber(sipData);
    const callType = SIPHeaderDecoder.determineCallType(sipData);
    const agentIdentifier = SIPHeaderDecoder.getAgentIdentifier(sipData);
    
    console.log(`\nüéØ [CALLER-ID-MATCH] ==========================================`);
    console.log(`üìû [CALLER-ID-MATCH] Call Type: ${callType}`);
    console.log(`üì± [CALLER-ID-MATCH] Customer Number: ${customerNumber}`);
    
    if (callType === 'outbound') {
      console.log(`üîç [CALLER-ID-MATCH] Searched for callerId: "${agentIdentifier}"`);
      console.log(`‚úÖ [CALLER-ID-MATCH] Matched callerId: "${agentConfig.callerId}"`);
      console.log(`üéØ [CALLER-ID-MATCH] Connection Status: SUCCESSFUL - Agent "${agentConfig.agentName}" connected`);
    } else {
      console.log(`üîç [CALLER-ID-MATCH] Searched for accountSid: "${agentIdentifier}"`);
      console.log(`‚úÖ [CALLER-ID-MATCH] Matched accountSid: "${agentConfig.accountSid}"`);
      console.log(`üéØ [CALLER-ID-MATCH] Connection Status: SUCCESSFUL - Agent "${agentConfig.agentName}" connected`);
    }
    
    console.log(`üè∑Ô∏è  [CALLER-ID-MATCH] Agent Details:`);
    console.log(`   ‚Ä¢ Name: ${agentConfig.agentName}`);
    console.log(`   ‚Ä¢ Client ID: ${agentConfig.clientId}`);
    console.log(`   ‚Ä¢ Language: ${agentConfig.language}`);
    console.log(`   ‚Ä¢ Voice: ${agentConfig.voiceSelection}`);
    console.log(`   ‚Ä¢ Category: ${agentConfig.category}`);
    console.log(`üéØ [CALLER-ID-MATCH] ==========================================\n`);
  }
}

// Main WebSocket server setup with enhanced call logging and SIP header parsing
const setupUnifiedVoiceServer = (wss) => {
  console.log("üöÄ [ENHANCED] Voice Server started with SIP header parsing and inbound/outbound call support");

  // Enhanced WebSocket connection handler with detailed SIP and agent logging
wss.on("connection", (ws, req) => {
  const connectionTime = new Date();
  const clientIP = req.socket.remoteAddress;
  
  console.log(`\nüîó [CONNECTION] ==========================================`);
  console.log(`üîó [CONNECTION] New enhanced WebSocket connection`);
  console.log(`üåê [CONNECTION] Client IP: ${clientIP}`);
  console.log(`‚è∞ [CONNECTION] Time: ${connectionTime.toISOString()}`);
  console.log(`üì° [CONNECTION] User Agent: ${req.headers['user-agent'] || 'unknown'}`);
  console.log(`üîó [CONNECTION] URL: ${req.url || 'unknown'}`);
  
  // Parse SIP data from connection URL (handle gracefully if not SIP)
  let sipData = null;
  try {
    if (req.url) {
      console.log(`üîç [CONNECTION] Parsing URL for SIP parameters...`);
      sipData = SIPHeaderDecoder.parseConnectionURL(req.url);
      
      if (sipData) {
        console.log(`‚úÖ [CONNECTION] SIP parameters detected - this is a SIP call`);
        SIPHeaderDecoder.logSIPData(sipData);
        ws.sipData = sipData; // Store SIP data in WebSocket session
        
        // Enhanced SIP data logging
        const customerNumber = SIPHeaderDecoder.getCustomerNumber(sipData);
        const callType = SIPHeaderDecoder.determineCallType(sipData);
        const agentIdentifier = SIPHeaderDecoder.getAgentIdentifier(sipData);
        
        console.log(`üéØ [CONNECTION] Call Analysis:`);
        console.log(`   ‚Ä¢ Customer Number: ${customerNumber}`);
        console.log(`   ‚Ä¢ Call Type: ${callType}`);
        console.log(`   ‚Ä¢ Agent Identifier: ${agentIdentifier}`);
        console.log(`   ‚Ä¢ DID: ${sipData.did}`);
        console.log(`   ‚Ä¢ Session ID: ${sipData.session_id}`);
        
        // Pre-validate agent existence (optional early check)
        console.log(`üîç [CONNECTION] Pre-checking agent configuration...`);
        AgentConfigFetcher.fetchAgentConfig(sipData)
          .then(result => {
            if (result.success) {
              console.log(`‚úÖ [PRE-CHECK] Agent "${result.agentConfig.agentName}" available for connection`);
              console.log(`‚úÖ [PRE-CHECK] Client ID: "${result.agentConfig.clientId}"`);
              console.log(`‚úÖ [PRE-CHECK] Language: "${result.agentConfig.language}"`);
            } else {
              console.warn(`‚ö†Ô∏è [PRE-CHECK] Agent configuration issue: ${result.error}`);
            }
          })
          .catch(error => {
            console.error(`‚ùå [PRE-CHECK] Error checking agent: ${error.message}`);
          });
          
      } else {
        console.log(`‚ÑπÔ∏è [CONNECTION] Non-SIP WebSocket connection (no SIP parameters found)`);
        console.log(`‚ÑπÔ∏è [CONNECTION] This might be a direct WebSocket connection or different protocol`);
      }
    } else {
      console.log(`‚ÑπÔ∏è [CONNECTION] No URL provided in connection request`);
    }
  } catch (error) {
    console.log(`‚ÑπÔ∏è [CONNECTION] Error parsing URL for SIP data: ${error.message}`);
    console.log(`‚ÑπÔ∏è [CONNECTION] Continuing with non-SIP connection handling`);
  }

  console.log(`üîó [CONNECTION] ==========================================\n`);

  // Session state
  let streamSid = null;
  let conversationHistory = [];
  let isProcessing = false;
  let userUtteranceBuffer = "";
  let lastProcessedText = "";
  let optimizedTTS = null;
  let currentLanguage = undefined;
  let processingRequestId = 0;
  let callLogger = null; // Call logger instance

  // Deepgram WebSocket connection
  let deepgramWs = null;
  let deepgramReady = false;
  let deepgramAudioQueue = [];

  // Optimized Deepgram connection
  const connectToDeepgram = async () => {
    try {
      const customerNumber = SIPHeaderDecoder.getCustomerNumber(sipData) || 'unknown';
      console.log(`üîå [DEEPGRAM] Connecting for customer ${customerNumber}...`);
      
      const deepgramLanguage = getDeepgramLanguage(currentLanguage);
      console.log(`üåç [DEEPGRAM] Using language: ${deepgramLanguage} for ${customerNumber}`);
      
      const deepgramUrl = new URL("wss://api.deepgram.com/v1/listen");
      deepgramUrl.searchParams.append("sample_rate", "8000");
      deepgramUrl.searchParams.append("channels", "1");
      deepgramUrl.searchParams.append("encoding", "linear16");
      deepgramUrl.searchParams.append("model", "nova-2");
      deepgramUrl.searchParams.append("language", deepgramLanguage);
      deepgramUrl.searchParams.append("interim_results", "true");
      deepgramUrl.searchParams.append("smart_format", "true");
      deepgramUrl.searchParams.append("endpointing", "300");

      deepgramWs = new WebSocket(deepgramUrl.toString(), {
        headers: { Authorization: `Token ${API_KEYS.deepgram}` },
      });

      deepgramWs.onopen = () => {
        deepgramReady = true;
        console.log(`‚úÖ [DEEPGRAM] Connected successfully for ${customerNumber}`);
        console.log(`üì¶ [DEEPGRAM] Processing ${deepgramAudioQueue.length} queued audio buffers`);
        
        deepgramAudioQueue.forEach((buffer, index) => {
          deepgramWs.send(buffer);
          console.log(`üì§ [DEEPGRAM] Sent queued buffer ${index + 1}/${deepgramAudioQueue.length} for ${customerNumber}`);
        });
        deepgramAudioQueue = [];
      };

      deepgramWs.onmessage = async (event) => {
        const data = JSON.parse(event.data);
        await handleDeepgramResponse(data);
      };

      deepgramWs.onerror = (error) => {
        console.error(`‚ùå [DEEPGRAM] Error for ${customerNumber}:`, error);
        deepgramReady = false;
      };

      deepgramWs.onclose = () => {
        console.log(`üîå [DEEPGRAM] Connection closed for ${customerNumber}`);
        deepgramReady = false;
      };

    } catch (error) {
      const customerNumber = SIPHeaderDecoder.getCustomerNumber(sipData) || 'unknown';
      console.error(`‚ùå [DEEPGRAM] Setup error for ${customerNumber}: ${error.message}`);
    }
  };

  // Handle Deepgram responses with enhanced logging
  const handleDeepgramResponse = async (data) => {
    const customerNumber = SIPHeaderDecoder.getCustomerNumber(sipData) || 'unknown';
    
    if (data.type === "Results") {
      const transcript = data.channel?.alternatives?.[0]?.transcript;
      const is_final = data.is_final;
      const confidence = data.channel?.alternatives?.[0]?.confidence;
      
      if (transcript?.trim()) {
        console.log(`üé§ [DEEPGRAM] ${is_final ? 'FINAL' : 'interim'} transcript from ${customerNumber}: "${transcript}" (confidence: ${confidence || 'unknown'})`);
        
        // Interrupt current TTS if new speech detected
        if (optimizedTTS && (isProcessing || optimizedTTS.isProcessing)) {
          console.log(`üõë [INTERRUPT] New speech from ${customerNumber} detected, interrupting current response`);
          optimizedTTS.interrupt();
          isProcessing = false;
          processingRequestId++; // Invalidate current processing
        }
        
        if (is_final) {
          userUtteranceBuffer += (userUtteranceBuffer ? " " : "") + transcript.trim();
          
          // Log the final transcript to call logger with SIP context
          if (callLogger && transcript.trim()) {
            const detectedLang = await detectLanguageWithOpenAI(transcript.trim());
            callLogger.logUserTranscript(transcript.trim(), detectedLang);
            
            // Enhanced logging with SIP data
            console.log(`üìù [TRANSCRIPT] Customer (${customerNumber}): "${transcript.trim()}" (${detectedLang})`);
          }
          
          await processUserUtterance(userUtteranceBuffer);
          userUtteranceBuffer = "";
        }
      }
    } else if (data.type === "UtteranceEnd") {
      console.log(`üîö [DEEPGRAM] Utterance end detected for ${customerNumber}`);
      
      if (userUtteranceBuffer.trim()) {
        // Log the utterance end transcript with SIP context
        if (callLogger && userUtteranceBuffer.trim()) {
          const detectedLang = await detectLanguageWithOpenAI(userUtteranceBuffer.trim());
          callLogger.logUserTranscript(userUtteranceBuffer.trim(), detectedLang);
          
          console.log(`üìù [UTTERANCE-END] Customer (${customerNumber}): "${userUtteranceBuffer.trim()}" (${detectedLang})`);
        }
        
        await processUserUtterance(userUtteranceBuffer);
        userUtteranceBuffer = "";
      }
    } else if (data.type === "Metadata") {
      console.log(`üìä [DEEPGRAM] Metadata for ${customerNumber}:`, {
        request_id: data.request_id,
        model_info: data.model_info
      });
    }
  };

  // Enhanced utterance processing with SIP context logging
  const processUserUtterance = async (text) => {
    if (!text.trim() || text === lastProcessedText) return;

    // Interrupt any ongoing processing
    if (optimizedTTS) {
      optimizedTTS.interrupt();
    }
    
    isProcessing = true;
    lastProcessedText = text;
    const currentRequestId = ++processingRequestId;
    const timer = createTimer("UTTERANCE_PROCESSING");

    try {
      const customerNumber = SIPHeaderDecoder.getCustomerNumber(sipData) || 'unknown';
      const callType = SIPHeaderDecoder.determineCallType(sipData);
      const agentName = ws.sessionAgentConfig?.agentName || 'unknown';
      
      console.log(`\nüé§ [USER] ==========================================`);
      console.log(`üé§ [USER] Processing utterance from ${customerNumber}`);
      console.log(`üìû [USER] Call Type: ${callType}`);
      console.log(`ü§ñ [USER] Agent: ${agentName}`);
      console.log(`üìù [USER] Text: "${text}"`);
      console.log(`üìç [USER] DID: ${sipData?.did || 'unknown'}`);
      console.log(`üÜî [USER] Session: ${sipData?.session_id || 'unknown'}`);

      // Step 1: Detect language using OpenAI
      const detectedLanguage = await detectLanguageWithOpenAI(text);
      
      // Step 2: Update current language and initialize TTS processor
      if (detectedLanguage !== currentLanguage) {
        console.log(`üåç [LANGUAGE] Changed: ${currentLanguage} ‚Üí ${detectedLanguage} for ${customerNumber}`);
        currentLanguage = detectedLanguage;
      }

      // Create new TTS processor with detected language
      optimizedTTS = new OptimizedSarvamTTSProcessor(detectedLanguage, ws, streamSid, callLogger);

      // Step 3: Check for interruption function
      const checkInterruption = () => {
        return processingRequestId !== currentRequestId;
      };

      console.log(`ü§ñ [PROCESSING] Starting OpenAI processing for ${customerNumber}...`);

      // Step 4: Process with OpenAI streaming
      const response = await processWithOpenAIStreaming(
        text,
        conversationHistory,
        detectedLanguage,
        (phrase, lang) => {
          // Handle phrase chunks - only if not interrupted
          if (processingRequestId === currentRequestId && !checkInterruption()) {
            console.log(`üì§ [PHRASE] "${phrase}" (${lang}) -> ${customerNumber}`);
            optimizedTTS.addPhrase(phrase, lang);
          }
        },
        (fullResponse) => {
          // Handle completion - only if not interrupted
          if (processingRequestId === currentRequestId && !checkInterruption()) {
            console.log(`‚úÖ [COMPLETE] "${fullResponse}" -> ${customerNumber}`);
            optimizedTTS.complete();
            
            const stats = optimizedTTS.getStats();
            console.log(`üìä [TTS-STATS] ${stats.totalChunks} chunks, ${stats.avgBytesPerChunk} avg bytes/chunk for ${customerNumber}`);
            
            // Update conversation history
            conversationHistory.push(
              { role: "user", content: text },
              { role: "assistant", content: fullResponse }
            );

            // Keep last 10 messages for context
            if (conversationHistory.length > 10) {
              conversationHistory = conversationHistory.slice(-10);
            }
          }
        },
        checkInterruption,
        callLogger // Pass call logger to OpenAI processing
      );

      console.log(`‚ö° [TOTAL] Processing completed in ${timer.end()}ms for ${customerNumber}`);
      console.log(`üé§ [USER] ==========================================\n`);

    } catch (error) {
      const customerNumber = SIPHeaderDecoder.getCustomerNumber(sipData) || 'unknown';
      console.error(`‚ùå [PROCESSING] Error for ${customerNumber}: ${error.message}`);
      console.error(`‚ùå [PROCESSING] Stack trace:`, error.stack);
    } finally {
      if (processingRequestId === currentRequestId) {
        isProcessing = false;
      }
    }
  };

    // Enhanced WebSocket message handling with better error handling and caller ID logging
ws.on("message", async (message) => {
  try {
    // Handle empty or invalid messages gracefully
    if (!message || message.length === 0) {
      const customerNumber = SIPHeaderDecoder.getCustomerNumber(sipData) || 'unknown';
      console.log(`‚ÑπÔ∏è [MESSAGE] Received empty message from ${customerNumber}`);
      return;
    }

    let messageString;
    try {
      messageString = message.toString();
      if (!messageString || messageString.trim() === '') {
        const customerNumber = SIPHeaderDecoder.getCustomerNumber(sipData) || 'unknown';
        console.log(`‚ÑπÔ∏è [MESSAGE] Received empty string message from ${customerNumber}`);
        return;
      }
    } catch (error) {
      console.log(`‚ö†Ô∏è [MESSAGE] Failed to convert message to string: ${error.message}`);
      return;
    }

    // Check if message is binary data (audio) vs JSON
    const isBinaryData = /[\x00-\x08\x0E-\x1F\x7F-\xFF]/.test(messageString);
    
    if (isBinaryData || messageString.includes('\x00') || messageString.includes('ÔøΩÔøΩ')) {
      // This is likely audio data, handle it as media
      const customerNumber = SIPHeaderDecoder.getCustomerNumber(sipData) || 'unknown';
      console.log(`üéµ [BINARY-DATA] Received binary audio data from ${customerNumber} (${messageString.length} bytes)`);
      
      // Try to extract base64 audio payload if this is a raw audio stream
      try {
        const audioBuffer = Buffer.from(message);
        console.log(`üéµ [AUDIO-STREAM] Processing raw audio buffer (${audioBuffer.length} bytes) from ${customerNumber}`);
        
        if (deepgramWs && deepgramReady && deepgramWs.readyState === WebSocket.OPEN) {
          deepgramWs.send(audioBuffer);
          console.log(`üì§ [AUDIO-STREAM] Sent ${audioBuffer.length} bytes to Deepgram for ${customerNumber}`);
        } else {
          deepgramAudioQueue.push(audioBuffer);
          console.log(`üì¶ [AUDIO-QUEUE] Queued ${audioBuffer.length} bytes for ${customerNumber} (Deepgram not ready)`);
        }
      } catch (audioError) {
        console.error(`‚ùå [AUDIO-STREAM] Error processing audio data from ${customerNumber}: ${audioError.message}`);
      }
      return;
    }

    let data;
    try {
      data = JSON.parse(messageString);
    } catch (jsonError) {
      // Enhanced error handling for different message types
      const customerNumber = SIPHeaderDecoder.getCustomerNumber(sipData) || 'unknown';
      
      // Check if it's a common non-JSON message type
      if (messageString.startsWith('<') || messageString.includes('HTTP/')) {
        console.log(`‚ÑπÔ∏è [MESSAGE] Received non-JSON message (likely HTTP/HTML) from ${customerNumber}: ${messageString.substring(0, 50)}...`);
        return;
      }
      
      // Check if it's WebRTC signaling data
      if (messageString.includes('candidate') || messageString.includes('sdp')) {
        console.log(`‚ÑπÔ∏è [MESSAGE] Received WebRTC signaling data from ${customerNumber}: ${messageString.substring(0, 50)}...`);
        return;
      }
      
      // Check if it's partial JSON (streaming)
      if (messageString.includes('{') || messageString.includes('}')) {
        console.log(`‚ö†Ô∏è [MESSAGE] Partial/malformed JSON from ${customerNumber}: ${jsonError.message}`);
        console.log(`‚ö†Ô∏è [MESSAGE] Raw message (first 200 chars): "${messageString.substring(0, 200)}..."`);
      } else {
        console.log(`‚ö†Ô∏è [MESSAGE] Non-JSON message from ${customerNumber}: "${messageString.substring(0, 100)}..."`);
      }
      return;
    }

    // Ensure data is an object with an event property
    if (!data || typeof data !== 'object' || !data.event) {
      const customerNumber = SIPHeaderDecoder.getCustomerNumber(sipData) || 'unknown';
      console.log(`‚ö†Ô∏è [MESSAGE] Invalid message format from ${customerNumber}:`, data);
      return;
    }

    let customerNumber;

    switch (data.event) {
      case "connected":
        customerNumber = SIPHeaderDecoder.getCustomerNumber(sipData) || 'unknown';
        console.log(`üîó [ENHANCED] Connected from ${customerNumber} - Protocol: ${data.protocol}`);
        console.log(`üîó [ENHANCED] Version: ${data.version || 'unknown'}`);
        break;

      case "start": {
        streamSid = data.streamSid || data.start?.streamSid;
        const accountSid = data.start?.accountSid;
        customerNumber = SIPHeaderDecoder.getCustomerNumber(sipData);
        const callType = SIPHeaderDecoder.determineCallType(sipData);
        
        console.log(`\nüéØ [ENHANCED] Stream started:`);
        console.log(`   ‚Ä¢ StreamSid: ${streamSid}`);
        console.log(`   ‚Ä¢ AccountSid: ${accountSid}`);
        console.log(`   ‚Ä¢ Customer Number: ${customerNumber}`);
        console.log(`   ‚Ä¢ Call Type: ${callType}`);
        
        if (sipData) {
          console.log(`   ‚Ä¢ SIP App ID: ${sipData.app_id}`);
          console.log(`   ‚Ä¢ SIP DID: ${sipData.did}`);
          console.log(`   ‚Ä¢ SIP Session ID: ${sipData.session_id}`);
          console.log(`   ‚Ä¢ SIP Direction: ${sipData.extra?.CallDirection || sipData.direction}`);
          console.log(`   ‚Ä¢ VA ID: ${sipData.extra?.CallVaId}`);
        }

        // Enhanced agent config fetching with detailed logging
        const fetchResult = await AgentConfigFetcher.fetchAgentConfig(sipData);
        
        if (!fetchResult.success) {
          console.error(`‚ùå [AGENT-CONFIG] ${fetchResult.error}`);
          
          // Send detailed error response
          const errorResponse = {
            event: 'error',
            message: fetchResult.error,
            details: {
              callType: fetchResult.callType,
              agentIdentifier: fetchResult.agentIdentifier,
              customerNumber: fetchResult.customerNumber,
              sipData: sipData
            }
          };
          
          ws.send(JSON.stringify(errorResponse));
          
          console.log(`üîå [AGENT-CONFIG] Closing connection due to missing agent configuration`);
          ws.close();
          return;
        }
        
        const agentConfig = fetchResult.agentConfig;
        const detectedCallType = fetchResult.callType;
        
        // Log successful caller ID match
        AgentConfigFetcher.logCallerIdConnection(sipData, agentConfig);
        
        // Validate agent configuration
        const validation = AgentConfigFetcher.validateAgentConfig(agentConfig);
        if (!validation.valid) {
          console.warn(`‚ö†Ô∏è [AGENT-VALIDATION] Agent configuration has issues but continuing...`);
        }
        
        console.log(`‚úÖ [AGENT-CONFIG] Successfully loaded for ${detectedCallType} call:`);
        console.log(`   ‚Ä¢ Client ID: ${agentConfig.clientId}`);
        console.log(`   ‚Ä¢ Agent Name: ${agentConfig.agentName}`);
        console.log(`   ‚Ä¢ Language: ${agentConfig.language}`);
        console.log(`   ‚Ä¢ Voice: ${agentConfig.voiceSelection}`);
        console.log(`   ‚Ä¢ STT: ${agentConfig.sttSelection}`);
        console.log(`   ‚Ä¢ TTS: ${agentConfig.ttsSelection}`);
        console.log(`   ‚Ä¢ LLM: ${agentConfig.llmSelection}`);
        
        if (detectedCallType === 'outbound') {
          console.log(`   ‚Ä¢ Caller ID: ${agentConfig.callerId}`);
        } else {
          console.log(`   ‚Ä¢ Account SID: ${agentConfig.accountSid}`);
        }
        
        ws.sessionAgentConfig = agentConfig;
        currentLanguage = agentConfig.language || 'hi';

        // Initialize enhanced call logger with SIP data
        callLogger = new CallLogger(agentConfig.clientId || accountSid, sipData);
        console.log(`üìù [CALL-LOG] Initialized for client: ${agentConfig.clientId}, customer: ${customerNumber}, call type: ${detectedCallType}`);

        // Log connection establishment with agent details
        console.log(`\nüéâ [CONNECTION-SUCCESS] ==========================================`);
        console.log(`‚úÖ [CONNECTION-SUCCESS] Agent "${agentConfig.agentName}" successfully connected!`);
        console.log(`üìû [CONNECTION-SUCCESS] Call Type: ${detectedCallType}`);
        console.log(`üì± [CONNECTION-SUCCESS] Customer: ${customerNumber}`);
        console.log(`üè∑Ô∏è  [CONNECTION-SUCCESS] Matched ID: ${detectedCallType === 'outbound' ? agentConfig.callerId : agentConfig.accountSid}`);
        console.log(`üéØ [CONNECTION-SUCCESS] ==========================================\n`);

        await connectToDeepgram();
        
        // Use agent's firstMessage for greeting and log it
        const greeting = agentConfig.firstMessage || "Hello! How can I help you today?";
        console.log(`üëã [GREETING] "${greeting}" -> ${customerNumber || 'unknown customer'} (${detectedCallType})`);
        
        // Log the initial greeting with enhanced context
        if (callLogger) {
          callLogger.logAIResponse(greeting, currentLanguage);
        }
        
        const tts = new OptimizedSarvamTTSProcessor(currentLanguage, ws, streamSid, callLogger);
        await tts.synthesizeAndStream(greeting);
        break;
      }

      case "media":
        customerNumber = SIPHeaderDecoder.getCustomerNumber(sipData) || 'unknown';
        
        if (data.media?.payload) {
          try {
            const audioBuffer = Buffer.from(data.media.payload, "base64");
            console.log(`üéµ [MEDIA] Received ${audioBuffer.length} bytes from ${customerNumber}`);
            
            if (deepgramWs && deepgramReady && deepgramWs.readyState === WebSocket.OPEN) {
              deepgramWs.send(audioBuffer);
              console.log(`üì§ [MEDIA] Sent to Deepgram for ${customerNumber}`);
            } else {
              deepgramAudioQueue.push(audioBuffer);
              console.log(`üì¶ [MEDIA] Queued for ${customerNumber} (${deepgramAudioQueue.length} items in queue)`);
              
              if (!deepgramReady) {
                console.log(`‚ö†Ô∏è [MEDIA] Deepgram not ready for ${customerNumber}, attempting reconnection...`);
                await connectToDeepgram();
              }
            }
          } catch (mediaError) {
            console.error(`‚ùå [MEDIA] Error processing media from ${customerNumber}: ${mediaError.message}`);
          }
        } else {
          console.log(`‚ö†Ô∏è [MEDIA] No payload in media message from ${customerNumber}`);
        }
        break;

      case "stop":
        customerNumber = SIPHeaderDecoder.getCustomerNumber(sipData) || 'unknown';
        const callType = SIPHeaderDecoder.determineCallType(sipData);
        console.log(`\nüìû [ENHANCED] Stream stopped for ${customerNumber} (${callType} call)`);
        
        // Enhanced call log saving with SIP context
        if (callLogger) {
          try {
            const savedLog = await callLogger.saveToDatabase('completed'); // Status for normal completion
            console.log(`üíæ [CALL-LOG] Final save completed - ID: ${savedLog._id}`);
            
            // Print enhanced call statistics with SIP data
            const stats = callLogger.getStats();
            console.log(`\nüìä [FINAL-STATS] Call Summary:`);
            console.log(`   ‚Ä¢ Duration: ${stats.duration}s`);
            console.log(`   ‚Ä¢ User Messages: ${stats.userMessages}`);
            console.log(`   ‚Ä¢ AI Responses: ${stats.aiResponses}`);
            console.log(`   ‚Ä¢ Languages: ${stats.languages.join(', ')}`);
            console.log(`   ‚Ä¢ Customer: ${customerNumber}`);
            console.log(`   ‚Ä¢ DID: ${sipData?.did || 'unknown'}`);
            console.log(`   ‚Ä¢ Call Type: ${stats.callType}`);
            console.log(`   ‚Ä¢ Session ID: ${sipData?.session_id || 'unknown'}`);
            
            if (sipData?.extra) {
              console.log(`   ‚Ä¢ VA ID: ${sipData.extra.CallVaId || 'unknown'}`);
              console.log(`   ‚Ä¢ Service App ID: ${sipData.extra.CZSERVICEAPPID || 'unknown'}`);
            }
            
          } catch (error) {
            console.error(`‚ùå [CALL-LOG] Failed to save final log: ${error.message}`);
          }
        }
        
        if (deepgramWs?.readyState === WebSocket.OPEN) {
          deepgramWs.close();
        }
        break;

      case "mark":
        customerNumber = SIPHeaderDecoder.getCustomerNumber(sipData) || 'unknown';
        console.log(`üèÅ [MARK] Received mark event from ${customerNumber}: ${data.mark?.name || 'unnamed'}`);
        break;

      default:
        customerNumber = SIPHeaderDecoder.getCustomerNumber(sipData) || 'unknown';
        console.log(`‚ùì [ENHANCED] Unknown event: ${data.event} from ${customerNumber}`);
        console.log(`‚ùì [ENHANCED] Event data:`, JSON.stringify(data, null, 2));
    }
  } catch (error) {
    const customerNumber = SIPHeaderDecoder.getCustomerNumber(sipData) || 'unknown';
    console.error(`‚ùå [ENHANCED] Unexpected error processing message from ${customerNumber}: ${error.message}`);
    console.error(`‚ùå [ENHANCED] Stack trace:`, error.stack);
    
    // Log the problematic message for debugging
    try {
      const messagePreview = message.toString().substring(0, 200);
      console.error(`‚ùå [ENHANCED] Problematic message preview: "${messagePreview}..."`);
    } catch (previewError) {
      console.error(`‚ùå [ENHANCED] Could not preview message: ${previewError.message}`);
    }
  }
});

    // Enhanced connection cleanup with SIP context
    // Enhanced connection cleanup with SIP context and detailed logging
  ws.on("close", async (code, reason) => {
    const customerNumber = SIPHeaderDecoder.getCustomerNumber(sipData) || 'unknown';
    const callType = SIPHeaderDecoder.determineCallType(sipData);
    const agentName = ws.sessionAgentConfig?.agentName || 'unknown';
    const connectionDuration = Date.now() - connectionTime.getTime();
    
    console.log(`\nüîó [DISCONNECT] ==========================================`);
    console.log(`üîó [DISCONNECT] Connection closed for ${customerNumber}`);
    console.log(`üìû [DISCONNECT] Call Type: ${callType}`);
    console.log(`ü§ñ [DISCONNECT] Agent: ${agentName}`);
    console.log(`‚è∞ [DISCONNECT] Connection Duration: ${Math.round(connectionDuration / 1000)}s`);
    console.log(`üî¢ [DISCONNECT] Close Code: ${code || 'unknown'}`);
    console.log(`üìù [DISCONNECT] Close Reason: ${reason || 'no reason provided'}`);
    
    if (sipData) {
      console.log(`üìç [DISCONNECT] DID: ${sipData.did}`);
      console.log(`üÜî [DISCONNECT] Session ID: ${sipData.session_id}`);
      console.log(`üè¢ [DISCONNECT] App ID: ${sipData.app_id}`);
      
      if (sipData.extra) {
        console.log(`üì± [DISCONNECT] VA ID: ${sipData.extra.CallVaId}`);
        console.log(`üîÑ [DISCONNECT] Call Direction: ${sipData.extra.CallDirection}`);
      }
    }
    
    // Save call log before cleanup if not already saved
    if (callLogger) {
      try {
        console.log(`üíæ [DISCONNECT] Saving call log for ${customerNumber}...`);
        
        // Determine disconnect reason for logging
        let disconnectReason = 'disconnected';
        if (code === 1000) disconnectReason = 'normal_closure';
        else if (code === 1001) disconnectReason = 'going_away';
        else if (code === 1006) disconnectReason = 'abnormal_closure';
        else if (code === 1011) disconnectReason = 'server_error';
        
        const savedLog = await callLogger.saveToDatabase(disconnectReason);
        
        console.log(`‚úÖ [DISCONNECT] Call log saved successfully`);
        console.log(`   ‚Ä¢ Log ID: ${savedLog._id}`);
        console.log(`   ‚Ä¢ Customer: ${customerNumber}`);
        console.log(`   ‚Ä¢ Agent: ${agentName}`);
        console.log(`   ‚Ä¢ Call Type: ${callType}`);
        console.log(`   ‚Ä¢ Status: ${disconnectReason}`);
        
        // Print final statistics
        const stats = callLogger.getStats();
        console.log(`üìä [DISCONNECT] Final Call Statistics:`);
        console.log(`   ‚Ä¢ Total Duration: ${stats.duration}s`);
        console.log(`   ‚Ä¢ User Messages: ${stats.userMessages}`);
        console.log(`   ‚Ä¢ AI Responses: ${stats.aiResponses}`);
        console.log(`   ‚Ä¢ Languages Used: ${stats.languages.join(', ')}`);
        console.log(`   ‚Ä¢ Customer Number: ${stats.customerNumber}`);
        console.log(`   ‚Ä¢ Call Type: ${stats.callType}`);
        
      } catch (error) {
        console.error(`‚ùå [DISCONNECT] Failed to save call log for ${customerNumber}: ${error.message}`);
        console.error(`‚ùå [DISCONNECT] Call log error details:`, error.stack);
      }
    } else {
      console.log(`‚ÑπÔ∏è [DISCONNECT] No call logger instance found for ${customerNumber}`);
    }
    
    // Cleanup Deepgram connection
    if (deepgramWs) {
      if (deepgramWs.readyState === WebSocket.OPEN) {
        console.log(`üîå [DISCONNECT] Closing Deepgram connection for ${customerNumber}`);
        deepgramWs.close();
      } else {
        console.log(`‚ÑπÔ∏è [DISCONNECT] Deepgram connection already closed for ${customerNumber}`);
      }
    }

    // Cleanup TTS processor
    if (optimizedTTS) {
      console.log(`üîä [DISCONNECT] Interrupting TTS processor for ${customerNumber}`);
      optimizedTTS.interrupt();
    }

    // Reset all state variables
    streamSid = null;
    conversationHistory = [];
    isProcessing = false;
    userUtteranceBuffer = "";
    lastProcessedText = "";
    deepgramReady = false;
    deepgramAudioQueue = [];
    optimizedTTS = null;
    currentLanguage = undefined;
    processingRequestId = 0;
    callLogger = null;
    
    console.log(`üßπ [DISCONNECT] Cleaned up all session data for ${customerNumber}`);
    console.log(`üîó [DISCONNECT] ==========================================\n`);
  });

  ws.on("error", (error) => {
    const customerNumber = SIPHeaderDecoder.getCustomerNumber(sipData) || 'unknown';
    const callType = SIPHeaderDecoder.determineCallType(sipData);
    const agentName = ws.sessionAgentConfig?.agentName || 'unknown';
    
    console.log(`\n‚ùå [ERROR] ==========================================`);
    console.error(`‚ùå [ERROR] WebSocket error for ${customerNumber}`);
    console.error(`üìû [ERROR] Call Type: ${callType}`);
    console.error(`ü§ñ [ERROR] Agent: ${agentName}`);
    console.error(`üìù [ERROR] Error Message: ${error.message}`);
    console.error(`üîç [ERROR] Error Code: ${error.code || 'unknown'}`);
    
    if (sipData) {
      console.error(`üìç [ERROR] DID: ${sipData.did}`);
      console.error(`üÜî [ERROR] Session ID: ${sipData.session_id}`);
    }
    
    console.error(`üìö [ERROR] Stack Trace:`, error.stack);
    console.log(`‚ùå [ERROR] ==========================================\n`);
    
    // Emergency call log save if error occurs
    if (callLogger) {
      callLogger.saveToDatabase('error').catch(logError => {
        console.error(`‚ùå [ERROR] Failed to save emergency call log: ${logError.message}`);
      });
    }
  });

  // Optional: Add ping/pong heartbeat for connection monitoring
  const heartbeatInterval = setInterval(() => {
    if (ws.readyState === WebSocket.OPEN) {
      const customerNumber = SIPHeaderDecoder.getCustomerNumber(sipData) || 'unknown';
      console.log(`üíì [HEARTBEAT] Connection alive for ${customerNumber}`);
      ws.ping();
    } else {
      clearInterval(heartbeatInterval);
    }
  }, 30000); // Every 30 seconds

  ws.on('pong', () => {
    const customerNumber = SIPHeaderDecoder.getCustomerNumber(sipData) || 'unknown';
    console.log(`üíì [PONG] Heartbeat response from ${customerNumber}`);
  });

  // Clear heartbeat on connection close
  ws.on('close', () => {
    clearInterval(heartbeatInterval);
  });
});
};

module.exports = { setupUnifiedVoiceServer, SIPHeaderDecoder, CallLogger, AgentConfigFetcher };