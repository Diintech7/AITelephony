const WebSocket = require("ws");
require("dotenv").config();
const mongoose = require('mongoose');
const Agent = require('../models/Agent');
const CallLog = require('../models/CallLog');

// Load API keys from environment variables
const API_KEYS = {
  deepgram: process.env.DEEPGRAM_API_KEY,
  sarvam: process.env.SARVAM_API_KEY,
  openai: process.env.OPENAI_API_KEY,
};

// Validate API keys
if (!API_KEYS.deepgram || !API_KEYS.sarvam || !API_KEYS.openai) {
  console.error("âŒ Missing required API keys in environment variables");
  process.exit(1);
}

const fetch = globalThis.fetch || require("node-fetch");

// Performance timing helper
const createTimer = (label) => {
  const start = Date.now();
  return {
    start,
    end: () => Date.now() - start,
    checkpoint: (checkpointName) => Date.now() - start,
  };
};

// Enhanced language mappings with Marathi support
const LANGUAGE_MAPPING = {
  hi: "hi-IN", 
  en: "en-IN", 
  bn: "bn-IN", 
  te: "te-IN", 
  ta: "ta-IN",
  mr: "mr-IN", // Marathi added
  gu: "gu-IN", 
  kn: "kn-IN", 
  ml: "ml-IN", 
  pa: "pa-IN",
  or: "or-IN", 
  as: "as-IN", 
  ur: "ur-IN",
};

const getSarvamLanguage = (detectedLang, defaultLang = "hi") => {
  const lang = detectedLang?.toLowerCase() || defaultLang;
  return LANGUAGE_MAPPING[lang] || "hi-IN";
};

const getDeepgramLanguage = (detectedLang, defaultLang = "hi") => {
  const lang = detectedLang?.toLowerCase() || defaultLang;
  if (lang === "hi") return "hi";
  if (lang === "en") return "en-IN";
  if (lang === "mr") return "mr"; // Marathi support for Deepgram
  return lang;
};

// Valid Sarvam voice options
const VALID_SARVAM_VOICES = ["meera", "pavithra", "arvind", "amol", "maya"];

const getValidSarvamVoice = (voiceSelection = "pavithra") => {
  if (VALID_SARVAM_VOICES.includes(voiceSelection)) {
    return voiceSelection;
  }
  
  const voiceMapping = {
    "male-professional": "arvind",
    "female-professional": "pavithra",
    "male-friendly": "amol",
    "female-friendly": "maya",
    neutral: "pavithra",
    default: "pavithra",
  };
  
  return voiceMapping[voiceSelection] || "pavithra";
};

// Enhanced language detection with Marathi support
const detectLanguageWithOpenAI = async (text) => {
  try {
    const response = await fetch("https://api.openai.com/v1/chat/completions", {
      method: "POST",
      headers: {
        "Content-Type": "application/json",
        Authorization: `Bearer ${API_KEYS.openai}`,
      },
      body: JSON.stringify({
        model: "gpt-4o-mini",
        messages: [
          {
            role: "system",
            content: `You are a language detection expert. Analyze the given text and return ONLY the 2-letter language code (hi, en, bn, te, ta, mr, gu, kn, ml, pa, or, as, ur). 

Examples:
- "Hello, how are you?" â†’ en
- "à¤¨à¤®à¤¸à¥à¤¤à¥‡, à¤†à¤ª à¤•à¥ˆà¤¸à¥‡ à¤¹à¥ˆà¤‚?" â†’ hi
- "à¦†à¦ªà¦¨à¦¿ à¦•à§‡à¦®à¦¨ à¦†à¦›à§‡à¦¨?" â†’ bn
- "à®¨à¯€à®™à¯à®•à®³à¯ à®à®ªà¯à®ªà®Ÿà®¿ à®‡à®°à¯à®•à¯à®•à®¿à®±à¯€à®°à¯à®•à®³à¯?" â†’ ta
- "à¤¤à¥à¤®à¥à¤¹à¥€ à¤•à¤¸à¥‡ à¤†à¤¹à¤¾à¤¤?" â†’ mr
- "àª¤àª®à«‡ àª•à«‡àª® àª›à«‹?" â†’ gu

Return only the language code, nothing else.`
          },
          {
            role: "user",
            content: text
          }
        ],
        max_tokens: 10,
        temperature: 0.1,
      }),
    });

    if (!response.ok) {
      throw new Error(`Language detection failed: ${response.status}`);
    }

    const data = await response.json();
    const detectedLang = data.choices[0]?.message?.content?.trim().toLowerCase();
    
    // Validate detected language
    const validLanguages = Object.keys(LANGUAGE_MAPPING);
    if (validLanguages.includes(detectedLang)) {
      console.log(`ğŸ” [LANG-DETECT] Detected: "${detectedLang}" from text: "${text.substring(0, 50)}..."`);
      return detectedLang;
    }
    
    console.log(`âš ï¸ [LANG-DETECT] Invalid language "${detectedLang}", defaulting to "hi"`);
    return "hi"; // Default fallback
    
  } catch (error) {
    console.error(`âŒ [LANG-DETECT] Error: ${error.message}`);
    return "hi"; // Default fallback
  }
};

// Call logging utility class
class CallLogger {
  constructor(clientId, mobile = null) {
    this.clientId = clientId;
    this.mobile = mobile;
    this.callStartTime = new Date();
    this.transcripts = [];
    this.responses = [];
    this.totalDuration = 0;
  }

  // Log user transcript from Deepgram
  logUserTranscript(transcript, language, timestamp = new Date()) {
    const entry = {
      type: 'user',
      text: transcript,
      language: language,
      timestamp: timestamp,
      source: 'deepgram'
    };
    
    this.transcripts.push(entry);
    console.log(`ğŸ“ [CALL-LOG] User: "${transcript}" (${language})`);
  }

  // Log AI response from Sarvam
  logAIResponse(response, language, timestamp = new Date()) {
    const entry = {
      type: 'ai',
      text: response,
      language: language,
      timestamp: timestamp,
      source: 'sarvam'
    };
    
    this.responses.push(entry);
    console.log(`ğŸ¤– [CALL-LOG] AI: "${response}" (${language})`);
  }

  // Generate full transcript combining user and AI messages
  generateFullTranscript() {
    const allEntries = [...this.transcripts, ...this.responses]
      .sort((a, b) => new Date(a.timestamp) - new Date(b.timestamp));
    
    return allEntries.map(entry => {
      const speaker = entry.type === 'user' ? 'User' : 'AI';
      const time = entry.timestamp.toISOString();
      return `[${time}] ${speaker} (${entry.language}): ${entry.text}`;
    }).join('\n');
  }

  // Save call log to database
  async saveToDatabase(leadStatus = 'medium') {
    try {
      const callEndTime = new Date();
      this.totalDuration = Math.round((callEndTime - this.callStartTime) / 1000); // Duration in seconds

      const callLogData = {
        clientId: this.clientId,
        mobile: this.mobile,
        time: this.callStartTime,
        transcript: this.generateFullTranscript(),
        duration: this.totalDuration,
        leadStatus: leadStatus,
        // Additional metadata
        metadata: {
          userTranscriptCount: this.transcripts.length,
          aiResponseCount: this.responses.length,
          languages: [...new Set([...this.transcripts, ...this.responses].map(entry => entry.language))],
          callEndTime: callEndTime
        }
      };

      const callLog = new CallLog(callLogData);
      const savedLog = await callLog.save();
      
      console.log(`ğŸ’¾ [CALL-LOG] Saved to DB - ID: ${savedLog._id}, Duration: ${this.totalDuration}s`);
      console.log(`ğŸ“Š [CALL-LOG] Stats - User messages: ${this.transcripts.length}, AI responses: ${this.responses.length}`);
      
      return savedLog;
    } catch (error) {
      console.error(`âŒ [CALL-LOG] Database save error: ${error.message}`);
      throw error;
    }
  }

  // Get call statistics
  getStats() {
    return {
      duration: this.totalDuration,
      userMessages: this.transcripts.length,
      aiResponses: this.responses.length,
      languages: [...new Set([...this.transcripts, ...this.responses].map(entry => entry.language))],
      startTime: this.callStartTime
    };
  }
}

// Optimized OpenAI streaming with phrase-based chunking and language detection
const processWithOpenAIStreaming = async (userMessage, conversationHistory, detectedLanguage, onPhrase, onComplete, onInterrupt, callLogger) => {
  const timer = createTimer("OPENAI_STREAMING");
  
  try {
    // Enhanced system prompt with Marathi support
    const getSystemPrompt = (lang) => {
      const prompts = {
        hi: "à¤†à¤ª à¤à¤†à¤ˆ à¤¤à¥‹à¤¤à¤¾ à¤¹à¥ˆà¤‚, à¤à¤• à¤µà¤¿à¤¨à¤®à¥à¤° à¤”à¤° à¤­à¤¾à¤µà¤¨à¤¾à¤¤à¥à¤®à¤• à¤°à¥‚à¤ª à¤¸à¥‡ à¤¬à¥à¤¦à¥à¤§à¤¿à¤®à¤¾à¤¨ AI à¤—à¥à¤°à¤¾à¤¹à¤• à¤¸à¥‡à¤µà¤¾ à¤•à¤¾à¤°à¥à¤¯à¤•à¤¾à¤°à¥€à¥¤ à¤†à¤ª à¤¹à¤¿à¤‚à¤¦à¥€ à¤®à¥‡à¤‚ à¤§à¤¾à¤°à¤¾à¤ªà¥à¤°à¤µà¤¾à¤¹ à¤¬à¥‹à¤²à¤¤à¥‡ à¤¹à¥ˆà¤‚à¥¤ à¤ªà¥à¤°à¤¾à¤•à¥ƒà¤¤à¤¿à¤•, à¤¬à¤¾à¤¤à¤šà¥€à¤¤ à¤•à¥€ à¤­à¤¾à¤·à¤¾ à¤•à¤¾ à¤ªà¥à¤°à¤¯à¥‹à¤— à¤•à¤°à¥‡à¤‚ à¤œà¥‹ à¤—à¤°à¥à¤®à¤œà¥‹à¤¶à¥€ à¤”à¤° à¤¸à¤¹à¤¾à¤¨à¥à¤­à¥‚à¤¤à¤¿ à¤¸à¥‡ à¤­à¤°à¥€ à¤¹à¥‹à¥¤ à¤œà¤µà¤¾à¤¬ à¤›à¥‹à¤Ÿà¥‡ à¤°à¤–à¥‡à¤‚â€”à¤•à¥‡à¤µà¤² 1-2 à¤²à¤¾à¤‡à¤¨à¥¤ à¤—à¥à¤°à¤¾à¤¹à¤•à¥‹à¤‚ à¤•à¥‹ à¤¸à¥à¤¨à¤¾, à¤¸à¤®à¤°à¥à¤¥à¤¿à¤¤ à¤”à¤° à¤®à¥‚à¤²à¥à¤¯à¤µà¤¾à¤¨ à¤®à¤¹à¤¸à¥‚à¤¸ à¤•à¤°à¤¾à¤¨à¤¾ à¤†à¤ªà¤•à¤¾ à¤²à¤•à¥à¤·à¥à¤¯ à¤¹à¥ˆà¥¤",
        
        en: "You are Aitota, a polite, emotionally intelligent AI customer care executive. You speak fluently in English. Use natural, conversational language with warmth and empathy. Keep responses shortâ€”just 1â€“2 lines. Your goal is to make customers feel heard, supported, and valued.",
        
        bn: "à¦†à¦ªà¦¨à¦¿ à¦†à¦‡à¦¤à§‹à¦¤à¦¾, à¦à¦•à¦œà¦¨ à¦­à¦¦à§à¦° à¦à¦¬à¦‚ à¦†à¦¬à§‡à¦—à¦ªà§à¦°à¦¬à¦£à¦­à¦¾à¦¬à§‡ à¦¬à§à¦¦à§à¦§à¦¿à¦®à¦¾à¦¨ AI à¦—à§à¦°à¦¾à¦¹à¦• à¦¸à§‡à¦¬à¦¾ à¦•à¦°à§à¦®à¦•à¦°à§à¦¤à¦¾à¥¤ à¦†à¦ªà¦¨à¦¿ à¦¬à¦¾à¦‚à¦²à¦¾à¦¯à¦¼ à¦¸à¦¾à¦¬à¦²à§€à¦²à¦­à¦¾à¦¬à§‡ à¦•à¦¥à¦¾ à¦¬à¦²à§‡à¦¨à¥¤ à¦‰à¦·à§à¦£à¦¤à¦¾ à¦à¦¬à¦‚ à¦¸à¦¹à¦¾à¦¨à§à¦­à§‚à¦¤à¦¿ à¦¸à¦¹ à¦ªà§à¦°à¦¾à¦•à§ƒà¦¤à¦¿à¦•, à¦•à¦¥à§‹à¦ªà¦•à¦¥à¦¨à¦®à§‚à¦²à¦• à¦­à¦¾à¦·à¦¾ à¦¬à§à¦¯à¦¬à¦¹à¦¾à¦° à¦•à¦°à§à¦¨à¥¤",
        
        te: "à°®à±€à°°à± à°à°¤à±‹à°¤à°¾, à°®à°°à±à°¯à°¾à°¦à°ªà±‚à°°à±à°µà°•, à°­à°¾à°µà±‹à°¦à±à°µà±‡à°—à°‚à°¤à±‹ à°¤à±†à°²à°¿à°µà±ˆà°¨ AI à°•à°¸à±à°Ÿà°®à°°à± à°•à±‡à°°à± à°à°—à±à°œà°¿à°•à±à°¯à±‚à°Ÿà°¿à°µà±. à°®à±€à°°à± à°¤à±†à°²à±à°—à±à°²à±‹ à°¸à°°à°³à°‚à°—à°¾ à°®à°¾à°Ÿà±à°²à°¾à°¡à±à°¤à°¾à°°à±à¥¤ à°µà±†à°šà±à°šà°¦à°¨à°‚ à°®à°°à°¿à°¯à± à°¸à°¾à°¨à±à°­à±‚à°¤à°¿à°¤à±‹ à°¸à°¹à°œà°®à±ˆà°¨, à°¸à°‚à°­à°¾à°·à°£à°¾ à°­à°¾à°·à°¨à± à°‰à°ªà°¯à±‹à°—à°¿à°‚à°šà°‚à°¡à°¿à¥¤",
        
        ta: "à®¨à¯€à®™à¯à®•à®³à¯ à®à®¤à¯‹à®¤à®¾, à®’à®°à¯ à®•à®£à¯à®£à®¿à®¯à®®à®¾à®©, à®‰à®£à®°à¯à®µà¯à®ªà¯‚à®°à¯à®µà®®à®¾à®• à®ªà¯à®¤à¯à®¤à®¿à®šà®¾à®²à®¿à®¤à¯à®¤à®©à®®à®¾à®© AI à®µà®¾à®Ÿà®¿à®•à¯à®•à¯ˆà®¯à®¾à®³à®°à¯ à®šà¯‡à®µà¯ˆ à®¨à®¿à®°à¯à®µà®¾à®•à®¿. à®¨à¯€à®™à¯à®•à®³à¯ à®¤à®®à®¿à®´à®¿à®²à¯ à®šà®°à®³à®®à®¾à®• à®ªà¯‡à®šà¯à®•à®¿à®±à¯€à®°à¯à®•à®³à¯. à®…à®©à¯à®ªà¯ à®®à®±à¯à®±à¯à®®à¯ à®…à®©à¯à®¤à®¾à®ªà®¤à¯à®¤à¯à®Ÿà®©à¯ à®‡à®¯à®±à¯à®•à¯ˆà®¯à®¾à®©, à®‰à®°à¯ˆà®¯à®¾à®Ÿà®²à¯ à®®à¯Šà®´à®¿à®¯à¯ˆà®ªà¯ à®ªà®¯à®©à¯à®ªà®Ÿà¯à®¤à¯à®¤à¯à®™à¯à®•à®³à¯à¥¤",
        
        mr: "à¤¤à¥à¤®à¥à¤¹à¥€ à¤à¤†à¤¯à¤¤à¥‹à¤¤à¤¾ à¤†à¤¹à¤¾à¤¤, à¤à¤• à¤¨à¤®à¥à¤° à¤†à¤£à¤¿ à¤­à¤¾à¤µà¤¨à¤¿à¤•à¤¦à¥ƒà¤·à¥à¤Ÿà¥à¤¯à¤¾ à¤¬à¥à¤¦à¥à¤§à¤¿à¤®à¤¾à¤¨ AI à¤—à¥à¤°à¤¾à¤¹à¤• à¤¸à¥‡à¤µà¤¾ à¤•à¤¾à¤°à¥à¤¯à¤•à¤¾à¤°à¥€. à¤¤à¥à¤®à¥à¤¹à¥€ à¤®à¤°à¤¾à¤ à¥€à¤¤ à¤…à¤¸à¥à¤–à¤²à¤¿à¤¤à¤ªà¤£à¥‡ à¤¬à¥‹à¤²à¤¤à¤¾. à¤‰à¤¬à¤¦à¤¾à¤°à¤ªà¤£à¤¾ à¤†à¤£à¤¿ à¤¸à¤¹à¤¾à¤¨à¥à¤­à¥‚à¤¤à¥€à¤¸à¤¹ à¤¨à¥ˆà¤¸à¤°à¥à¤—à¤¿à¤•, à¤¸à¤‚à¤­à¤¾à¤·à¤£à¤¾à¤¤à¥à¤®à¤• à¤­à¤¾à¤·à¤¾ à¤µà¤¾à¤ªà¤°à¤¾. à¤‰à¤¤à¥à¤¤à¤°à¥‡ à¤²à¤¹à¤¾à¤¨ à¤ à¥‡à¤µà¤¾â€”à¤«à¤•à¥à¤¤ 1-2 à¤“à¤³à¥€. à¤—à¥à¤°à¤¾à¤¹à¤•à¤¾à¤‚à¤¨à¤¾ à¤à¤•à¤²à¥‡, à¤¸à¤®à¤°à¥à¤¥à¤¿à¤¤ à¤†à¤£à¤¿ à¤®à¥‚à¤²à¥à¤¯à¤µà¤¾à¤¨ à¤µà¤¾à¤Ÿà¤£à¥à¤¯à¤¾à¤šà¥‡ à¤¤à¥à¤®à¤šà¥‡ à¤§à¥à¤¯à¥‡à¤¯ à¤†à¤¹à¥‡à¥¤"
      };
      
      return prompts[lang] || prompts.en;
    };

    const systemPrompt = getSystemPrompt(detectedLanguage);

    const messages = [
      { role: "system", content: systemPrompt },
      ...conversationHistory.slice(-6),
      { role: "user", content: userMessage }
    ];

    const response = await fetch("https://api.openai.com/v1/chat/completions", {
      method: "POST",
      headers: {
        "Content-Type": "application/json",
        Authorization: `Bearer ${API_KEYS.openai}`,
      },
      body: JSON.stringify({
        model: "gpt-4o-mini",
        messages,
        max_tokens: 80,
        temperature: 0.3,
        stream: true,
      }),
    });

    if (!response.ok) {
      console.error(`âŒ [OPENAI] Error: ${response.status}`);
      return null;
    }

    let fullResponse = "";
    let phraseBuffer = "";
    let isFirstPhrase = true;
    let isInterrupted = false;

    const reader = response.body.getReader();
    const decoder = new TextDecoder();

    // Check for interruption periodically
    const checkInterruption = () => {
      return onInterrupt && onInterrupt();
    };

    while (true) {
      // Check for interruption
      if (checkInterruption()) {
        isInterrupted = true;
        console.log(`âš ï¸ [OPENAI] Stream interrupted by new user input`);
        reader.cancel();
        break;
      }

      const { done, value } = await reader.read();
      if (done) break;

      const chunk = decoder.decode(value, { stream: true });
      const lines = chunk.split('\n').filter(line => line.trim());

      for (const line of lines) {
        if (line.startsWith('data: ')) {
          const data = line.slice(6);
          
          if (data === '[DONE]') {
            if (phraseBuffer.trim() && !isInterrupted) {
              onPhrase(phraseBuffer.trim(), detectedLanguage);
              fullResponse += phraseBuffer;
            }
            break;
          }

          try {
            const parsed = JSON.parse(data);
            const content = parsed.choices?.[0]?.delta?.content;
            
            if (content) {
              phraseBuffer += content;
              
              // Check for interruption before sending phrase
              if (checkInterruption()) {
                isInterrupted = true;
                break;
              }
              
              if (shouldSendPhrase(phraseBuffer)) {
                const phrase = phraseBuffer.trim();
                if (phrase.length > 0 && !isInterrupted) {
                  if (isFirstPhrase) {
                    console.log(`âš¡ [OPENAI] First phrase (${timer.checkpoint('first_phrase')}ms)`);
                    isFirstPhrase = false;
                  }
                  onPhrase(phrase, detectedLanguage);
                  fullResponse += phrase;
                  phraseBuffer = "";
                }
              }
            }
          } catch (e) {
            // Skip malformed JSON
          }
        }
      }
      
      if (isInterrupted) break;
    }

    if (!isInterrupted) {
      console.log(`ğŸ¤– [OPENAI] Complete: "${fullResponse}" (${timer.end()}ms)`);
      
      // Log AI response to call logger
      if (callLogger && fullResponse.trim()) {
        callLogger.logAIResponse(fullResponse.trim(), detectedLanguage);
      }
      
      onComplete(fullResponse);
    } else {
      console.log(`ğŸ¤– [OPENAI] Interrupted after ${timer.end()}ms`);
    }
    
    return isInterrupted ? null : fullResponse;

  } catch (error) {
    console.error(`âŒ [OPENAI] Error: ${error.message}`);
    return null;
  }
};

// Smart phrase detection for better chunking
const shouldSendPhrase = (buffer) => {
  const trimmed = buffer.trim();
  
  // Complete sentences
  if (/[.!?à¥¤à¥¥à¥¤]$/.test(trimmed)) return true;
  
  // Meaningful phrases with natural breaks
  if (trimmed.length >= 8 && /[,;à¥¤]\s*$/.test(trimmed)) return true;
  
  // Longer phrases (prevent too much buffering)
  if (trimmed.length >= 25 && /\s/.test(trimmed)) return true;
  
  return false;
};

// Enhanced TTS processor with call logging
class OptimizedSarvamTTSProcessor {
  constructor(language, ws, streamSid, callLogger = null) {
    this.language = language;
    this.ws = ws;
    this.streamSid = streamSid;
    this.callLogger = callLogger;
    this.queue = [];
    this.isProcessing = false;
    this.sarvamLanguage = getSarvamLanguage(language);
    this.voice = getValidSarvamVoice(ws.sessionAgentConfig?.voiceSelection || "pavithra");
    
    // Interruption handling
    this.isInterrupted = false;
    this.currentAudioStreaming = null;
    
    // Sentence-based processing settings
    this.sentenceBuffer = "";
    this.processingTimeout = 100;
    this.sentenceTimer = null;
    
    // Audio streaming stats
    this.totalChunks = 0;
    this.totalAudioBytes = 0;
  }

  // Method to interrupt current processing
  interrupt() {
    console.log(`âš ï¸ [SARVAM-TTS] Interrupting current processing`);
    this.isInterrupted = true;
    
    // Clear queue and buffer
    this.queue = [];
    this.sentenceBuffer = "";
    
    // Clear any pending timeout
    if (this.sentenceTimer) {
      clearTimeout(this.sentenceTimer);
      this.sentenceTimer = null;
    }
    
    // Stop current audio streaming if active
    if (this.currentAudioStreaming) {
      this.currentAudioStreaming.interrupt = true;
    }
    
    console.log(`ğŸ›‘ [SARVAM-TTS] Processing interrupted and cleaned up`);
  }

  // Reset for new processing
  reset(newLanguage) {
    this.interrupt();
    
    // Update language settings
    if (newLanguage) {
      this.language = newLanguage;
      this.sarvamLanguage = getSarvamLanguage(newLanguage);
      console.log(`ğŸ”„ [SARVAM-TTS] Language updated to: ${this.sarvamLanguage}`);
    }
    
    // Reset state
    this.isInterrupted = false;
    this.isProcessing = false;
    this.totalChunks = 0;
    this.totalAudioBytes = 0;
  }

  addPhrase(phrase, detectedLanguage) {
    if (!phrase.trim() || this.isInterrupted) return;
    
    // Update language if different from current
    if (detectedLanguage && detectedLanguage !== this.language) {
      console.log(`ğŸ”„ [SARVAM-TTS] Language change detected: ${this.language} â†’ ${detectedLanguage}`);
      this.language = detectedLanguage;
      this.sarvamLanguage = getSarvamLanguage(detectedLanguage);
    }
    
    this.sentenceBuffer += (this.sentenceBuffer ? " " : "") + phrase.trim();
    
    if (this.hasCompleteSentence(this.sentenceBuffer)) {
      this.processCompleteSentences();
    } else {
      this.scheduleProcessing();
    }
  }

  hasCompleteSentence(text) {
    return /[.!?à¥¤à¥¥à¥¤]/.test(text);
  }

  extractCompleteSentences(text) {
    const sentences = text.split(/([.!?à¥¤à¥¥à¥¤])/).filter(s => s.trim());
    
    let completeSentences = "";
    let remainingText = "";
    
    for (let i = 0; i < sentences.length; i += 2) {
      const sentence = sentences[i];
      const punctuation = sentences[i + 1];
      
      if (punctuation) {
        completeSentences += sentence + punctuation + " ";
      } else {
        remainingText = sentence;
      }
    }
    
    return {
      complete: completeSentences.trim(),
      remaining: remainingText.trim()
    };
  }

  processCompleteSentences() {
    if (this.isInterrupted) return;
    
    if (this.sentenceTimer) {
      clearTimeout(this.sentenceTimer);
      this.sentenceTimer = null;
    }

    const { complete, remaining } = this.extractCompleteSentences(this.sentenceBuffer);
    
    if (complete && !this.isInterrupted) {
      this.queue.push(complete);
      this.sentenceBuffer = remaining;
      this.processQueue();
    }
  }

  scheduleProcessing() {
    if (this.isInterrupted) return;
    
    if (this.sentenceTimer) clearTimeout(this.sentenceTimer);
    
    this.sentenceTimer = setTimeout(() => {
      if (this.sentenceBuffer.trim() && !this.isInterrupted) {
        this.queue.push(this.sentenceBuffer.trim());
        this.sentenceBuffer = "";
        this.processQueue();
      }
    }, this.processingTimeout);
  }

  async processQueue() {
    if (this.isProcessing || this.queue.length === 0 || this.isInterrupted) return;

    this.isProcessing = true;
    const textToProcess = this.queue.shift();

    try {
      if (!this.isInterrupted) {
        await this.synthesizeAndStream(textToProcess);
      }
    } catch (error) {
      if (!this.isInterrupted) {
        console.error(`âŒ [SARVAM-TTS] Error: ${error.message}`);
      }
    } finally {
      this.isProcessing = false;
      
      // Process next item in queue if not interrupted
      if (this.queue.length > 0 && !this.isInterrupted) {
        setTimeout(() => this.processQueue(), 10);
      }
    }
  }

  async synthesizeAndStream(text) {
    if (this.isInterrupted) return;
    
    const timer = createTimer("SARVAM_TTS_SENTENCE");
    
    try {
      console.log(`ğŸµ [SARVAM-TTS] Synthesizing: "${text}" (${this.sarvamLanguage})`);

      const response = await fetch("https://api.sarvam.ai/text-to-speech", {
        method: "POST",
        headers: {
          "Content-Type": "application/json",
          "API-Subscription-Key": API_KEYS.sarvam,
        },
        body: JSON.stringify({
          inputs: [text],
          target_language_code: this.sarvamLanguage,
          speaker: this.voice,
          pitch: 0,
          pace: 1.0,
          loudness: 1.0,
          speech_sample_rate: 8000,
          enable_preprocessing: false,
          model: "bulbul:v1",
        }),
      });

      if (!response.ok || this.isInterrupted) {
        if (this.isInterrupted) return;
        throw new Error(`Sarvam API error: ${response.status} - ${response.statusText}`);
      }

      const responseData = await response.json();
      const audioBase64 = responseData.audios?.[0];
      
      if (!audioBase64 || this.isInterrupted) {
        if (!this.isInterrupted) {
          throw new Error("No audio data received from Sarvam API");
        }
        return;
      }

      console.log(`âš¡ [SARVAM-TTS] Synthesis completed in ${timer.end()}ms`);
      
      // Stream audio if not interrupted
      if (!this.isInterrupted) {
        await this.streamAudioOptimizedForSIP(audioBase64);
        
        const audioBuffer = Buffer.from(audioBase64, "base64");
        this.totalAudioBytes += audioBuffer.length;
        this.totalChunks++;
      }
      
    } catch (error) {
      if (!this.isInterrupted) {
        console.error(`âŒ [SARVAM-TTS] Synthesis error: ${error.message}`);
        throw error;
      }
    }
  }

  async streamAudioOptimizedForSIP(audioBase64) {
    if (this.isInterrupted) return;
    
    const audioBuffer = Buffer.from(audioBase64, "base64");
    const streamingSession = { interrupt: false };
    this.currentAudioStreaming = streamingSession;
    
    // SIP audio specifications
    const SAMPLE_RATE = 8000;
    const BYTES_PER_SAMPLE = 2;
    const BYTES_PER_MS = (SAMPLE_RATE * BYTES_PER_SAMPLE) / 1000;
    const OPTIMAL_CHUNK_SIZE = Math.floor(40 * BYTES_PER_MS);
    
    console.log(`ğŸ“¦ [SARVAM-SIP] Streaming ${audioBuffer.length} bytes`);
    
    let position = 0;
    let chunkIndex = 0;
    
    while (position < audioBuffer.length && !this.isInterrupted && !streamingSession.interrupt) {
      const remaining = audioBuffer.length - position;
      const chunkSize = Math.min(OPTIMAL_CHUNK_SIZE, remaining);
      const chunk = audioBuffer.slice(position, position + chunkSize);
      
      console.log(`ğŸ“¤ [SARVAM-SIP] Chunk ${chunkIndex + 1}: ${chunk.length} bytes`);
      
      const mediaMessage = {
        event: "media",
        streamSid: this.streamSid,
        media: {
          payload: chunk.toString("base64")
        }
      };

      if (this.ws.readyState === WebSocket.OPEN && !this.isInterrupted) {
        this.ws.send(JSON.stringify(mediaMessage));
      }
      
      // Delay between chunks
      if (position + chunkSize < audioBuffer.length && !this.isInterrupted) {
        const chunkDurationMs = Math.floor(chunk.length / BYTES_PER_MS);
        const delayMs = Math.max(chunkDurationMs - 2, 10);
        await new Promise(resolve => setTimeout(resolve, delayMs));
      }
      
      position += chunkSize;
      chunkIndex++;
    }
    
    if (this.isInterrupted || streamingSession.interrupt) {
      console.log(`ğŸ›‘ [SARVAM-SIP] Audio streaming interrupted at chunk ${chunkIndex}`);
    } else {
      console.log(`âœ… [SARVAM-SIP] Completed streaming ${chunkIndex} chunks`);
    }
    
    this.currentAudioStreaming = null;
  }

  complete() {
    if (this.isInterrupted) return;
    
    if (this.sentenceBuffer.trim()) {
      this.queue.push(this.sentenceBuffer.trim());
      this.sentenceBuffer = "";
    }
    
    if (this.queue.length > 0) {
      this.processQueue();
    }
    
    console.log(`ğŸ“Š [SARVAM-STATS] Total: ${this.totalChunks} sentences, ${this.totalAudioBytes} bytes`);
  }

  getStats() {
    return {
      totalChunks: this.totalChunks,
      totalAudioBytes: this.totalAudioBytes,
      avgBytesPerChunk: this.totalChunks > 0 ? Math.round(this.totalAudioBytes / this.totalChunks) : 0
    };
  }
}

// Main WebSocket server setup with enhanced call logging
const setupUnifiedVoiceServer = (wss) => {
  console.log("ğŸš€ [ENHANCED] Voice Server started with call logging and Marathi support");

  wss.on("connection", (ws, req) => {
    console.log("ğŸ”— [CONNECTION] New enhanced WebSocket connection");
    
    // ===== ENHANCED LOGGING - Connection Details =====
    const connectionDetails = {
      timestamp: new Date().toISOString(),
      clientIP: req.socket.remoteAddress,
      userAgent: req.headers["user-agent"],
      origin: req.headers.origin,
      host: req.headers.host,
      protocol: req.headers["sec-websocket-protocol"],
      extensions: req.headers["sec-websocket-extensions"],
      key: req.headers["sec-websocket-key"],
      version: req.headers["sec-websocket-version"],
      url: req.url,
      method: req.method,
      headers: req.headers
    };
    
    console.log(`ğŸ“‹ [WS-CONNECTION] Full Connection Details:`);
    console.log(`   ğŸ• Timestamp: ${connectionDetails.timestamp}`);
    console.log(`   ğŸŒ Client IP: ${connectionDetails.clientIP}`);
    console.log(`   ğŸ”— URL: ${connectionDetails.url}`);
    console.log(`   ğŸ“± User Agent: ${connectionDetails.userAgent}`);
    console.log(`   ğŸ  Origin: ${connectionDetails.origin}`);
    console.log(`   ğŸ–¥ï¸  Host: ${connectionDetails.host}`);
    console.log(`   ğŸ“„ Method: ${connectionDetails.method}`);
    console.log(`   ğŸ”§ Protocol: ${connectionDetails.protocol}`);
    console.log(`   ğŸ“¦ Extensions: ${connectionDetails.extensions}`);
    console.log(`   ğŸ”‘ WebSocket Key: ${connectionDetails.key}`);
    console.log(`   ğŸ“Š WebSocket Version: ${connectionDetails.version}`);
    console.log(`   ğŸ“‹ All Headers:`, JSON.stringify(connectionDetails.headers, null, 2));
    console.log(`â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”`);

    // Session state
    let streamSid = null;
    let conversationHistory = [];
    let isProcessing = false;
    let userUtteranceBuffer = "";
    let lastProcessedText = "";
    let optimizedTTS = null;
    let currentLanguage = undefined;
    let processingRequestId = 0;
    let callLogger = null; // Call logger instance

    // Deepgram WebSocket connection
    let deepgramWs = null;
    let deepgramReady = false;
    let deepgramAudioQueue = [];

    // Optimized Deepgram connection
    const connectToDeepgram = async () => {
      try {
        console.log("ğŸ”Œ [DEEPGRAM] Connecting...");
        const deepgramLanguage = getDeepgramLanguage(currentLanguage);
        
        const deepgramUrl = new URL("wss://api.deepgram.com/v1/listen");
        deepgramUrl.searchParams.append("sample_rate", "8000");
        deepgramUrl.searchParams.append("channels", "1");
        deepgramUrl.searchParams.append("encoding", "linear16");
        deepgramUrl.searchParams.append("model", "nova-2");
        deepgramUrl.searchParams.append("language", deepgramLanguage);
        deepgramUrl.searchParams.append("interim_results", "true");
        deepgramUrl.searchParams.append("smart_format", "true");
        deepgramUrl.searchParams.append("endpointing", "300");

        deepgramWs = new WebSocket(deepgramUrl.toString(), {
          headers: { Authorization: `Token ${API_KEYS.deepgram}` },
        });

        deepgramWs.onopen = () => {
          deepgramReady = true;
          console.log("âœ… [DEEPGRAM] Connected");
          
          deepgramAudioQueue.forEach(buffer => deepgramWs.send(buffer));
          deepgramAudioQueue = [];
        };

        deepgramWs.onmessage = async (event) => {
          const data = JSON.parse(event.data);
          await handleDeepgramResponse(data);
        };

        deepgramWs.onerror = (error) => {
          console.error("âŒ [DEEPGRAM] Error:", error);
          deepgramReady = false;
        };

        deepgramWs.onclose = () => {
          console.log("ğŸ”Œ [DEEPGRAM] Connection closed");
          deepgramReady = false;
        };

      } catch (error) {
        console.error("âŒ [DEEPGRAM] Setup error:", error.message);
      }
    };

    // Handle Deepgram responses with call logging
    const handleDeepgramResponse = async (data) => {
      if (data.type === "Results") {
        const transcript = data.channel?.alternatives?.[0]?.transcript;
        const is_final = data.is_final;
        
        if (transcript?.trim()) {
          // Interrupt current TTS if new speech detected
          if (optimizedTTS && (isProcessing || optimizedTTS.isProcessing)) {
            console.log(`ğŸ›‘ [INTERRUPT] New speech detected, interrupting current response`);
            optimizedTTS.interrupt();
            isProcessing = false;
            processingRequestId++; // Invalidate current processing
          }
          
          if (is_final) {
            userUtteranceBuffer += (userUtteranceBuffer ? " " : "") + transcript.trim();
            
            // Log the final transcript to call logger
            if (callLogger && transcript.trim()) {
              const detectedLang = await detectLanguageWithOpenAI(transcript.trim());
              callLogger.logUserTranscript(transcript.trim(), detectedLang);
            }
            
            await processUserUtterance(userUtteranceBuffer);
            userUtteranceBuffer = "";
          }
        }
      } else if (data.type === "UtteranceEnd") {
        if (userUtteranceBuffer.trim()) {
          // Log the utterance end transcript
          if (callLogger && userUtteranceBuffer.trim()) {
            const detectedLang = await detectLanguageWithOpenAI(userUtteranceBuffer.trim());
            callLogger.logUserTranscript(userUtteranceBuffer.trim(), detectedLang);
          }
          
          await processUserUtterance(userUtteranceBuffer);
          userUtteranceBuffer = "";
        }
      }
    };

    // Enhanced utterance processing with call logging
    const processUserUtterance = async (text) => {
      if (!text.trim() || text === lastProcessedText) return;

      // Interrupt any ongoing processing
      if (optimizedTTS) {
        optimizedTTS.interrupt();
      }
      
      isProcessing = true;
      lastProcessedText = text;
      const currentRequestId = ++processingRequestId;
      const timer = createTimer("UTTERANCE_PROCESSING");

      try {
        console.log(`ğŸ¤ [USER] Processing: "${text}"`);

        // Step 1: Detect language using OpenAI
        const detectedLanguage = await detectLanguageWithOpenAI(text);
        
        // Step 2: Update current language and initialize TTS processor
        if (detectedLanguage !== currentLanguage) {
          console.log(`ğŸŒ [LANGUAGE] Changed: ${currentLanguage} â†’ ${detectedLanguage}`);
          currentLanguage = detectedLanguage;
        }

        // Create new TTS processor with detected language
        optimizedTTS = new OptimizedSarvamTTSProcessor(detectedLanguage, ws, streamSid, callLogger);

        // Step 3: Check for interruption function
        const checkInterruption = () => {
          return processingRequestId !== currentRequestId;
        };

        // Step 4: Process with OpenAI streaming
        const response = await processWithOpenAIStreaming(
          text,
          conversationHistory,
          detectedLanguage,
          (phrase, lang) => {
            // Handle phrase chunks - only if not interrupted
            if (processingRequestId === currentRequestId && !checkInterruption()) {
              console.log(`ğŸ“¤ [PHRASE] "${phrase}" (${lang})`);
              optimizedTTS.addPhrase(phrase, lang);
            }
          },
          (fullResponse) => {
            // Handle completion - only if not interrupted
            if (processingRequestId === currentRequestId && !checkInterruption()) {
              console.log(`âœ… [COMPLETE] "${fullResponse}"`);
              optimizedTTS.complete();
              
              const stats = optimizedTTS.getStats();
              console.log(`ğŸ“Š [TTS-STATS] ${stats.totalChunks} chunks, ${stats.avgBytesPerChunk} avg bytes/chunk`);
              
              // Update conversation history
              conversationHistory.push(
                { role: "user", content: text },
                { role: "assistant", content: fullResponse }
              );

              // Keep last 10 messages for context
              if (conversationHistory.length > 10) {
                conversationHistory = conversationHistory.slice(-10);
              }
            }
          },
          checkInterruption,
          callLogger // Pass call logger to OpenAI processing
        );

        console.log(`âš¡ [TOTAL] Processing time: ${timer.end()}ms`);

      } catch (error) {
        console.error(`âŒ [PROCESSING] Error: ${error.message}`);
      } finally {
        if (processingRequestId === currentRequestId) {
          isProcessing = false;
        }
      }
    };

    // ===== ENHANCED LOGGING - WebSocket Message Handling =====
    ws.on("message", async (message) => {
      try {
        // Log raw message details
        console.log(`\nğŸ“¨ [WS-INCOMING] New Message Received:`);
        console.log(`   ğŸ• Timestamp: ${new Date().toISOString()}`);
        console.log(`   ğŸ“ Message Size: ${message.length} bytes`);
        console.log(`   ğŸ“¦ Message Type: ${typeof message}`);
        console.log(`   ğŸ”¤ Message Buffer Type: ${Buffer.isBuffer(message) ? 'Buffer' : 'String'}`);
        
        // Try to parse as JSON and log both raw and parsed data
        let data;
        let rawMessageString = message.toString();
        
        console.log(`   ğŸ“„ Raw Message (first 500 chars): ${rawMessageString.substring(0, 500)}${rawMessageString.length > 500 ? '...' : ''}`);
        
        try {
          data = JSON.parse(rawMessageString);
          console.log(`   âœ… Successfully parsed JSON`);
          console.log(`   ğŸ“‹ Parsed Data:`, JSON.stringify(data, null, 2));
        } catch (parseError) {
          console.log(`   âŒ JSON Parse Error: ${parseError.message}`);
          console.log(`   ğŸ”¤ Raw String: "${rawMessageString}"`);
          return;
        }

        // Log event-specific details
        console.log(`   ğŸ¯ Event Type: "${data.event}"`);
        
        switch (data.event) {
          case "connected":
            console.log(`   ğŸ”— [EVENT-CONNECTED] Protocol: ${data.protocol}`);
            console.log(`   ğŸ“‹ Full Connected Data:`, JSON.stringify(data, null, 2));
            break;

          case "start": {
            streamSid = data.streamSid || data.start?.streamSid;
            const accountSid = data.start?.accountSid;
            const mobile = data.start?.from || null; // Extract mobile number from call data
            
            console.log(`   ğŸ¯ [EVENT-START] Stream Details:`);
            console.log(`      ğŸ“¡ StreamSid: ${streamSid}`);
            console.log(`      ğŸ¢ AccountSid: ${accountSid}`);
            console.log(`      ğŸ“± Mobile: ${mobile}`);
            console.log(`      ğŸ“‹ Complete Start Data:`, JSON.stringify(data, null, 2));

            // Fetch agent config from DB using accountSid (MANDATORY)
            let agentConfig = null;
            if (accountSid) {
              try {
                console.log(`   ğŸ” [DB-QUERY] Searching for agent with accountSid: ${accountSid}`);
                agentConfig = await Agent.findOne({ accountSid }).lean();
                if (!agentConfig) {
                  console.log(`   âŒ [DB-ERROR] No agent found for accountSid: ${accountSid}`);
                  ws.send(JSON.stringify({ event: 'error', message: `No agent found for accountSid: ${accountSid}` }));
                  ws.close();
                  return;
                } else {
                  console.log(`   âœ… [DB-SUCCESS] Agent found:`, JSON.stringify(agentConfig, null, 2));
                }
                
              } catch (err) {
                console.log(`   âŒ [DB-ERROR] Database error for accountSid: ${accountSid}`, err);
                ws.send(JSON.stringify({ event: 'error', message: `DB error for accountSid: ${accountSid}` }));
                ws.close();
                return;
              }
            } else {
              console.log(`   âŒ [VALIDATION-ERROR] Missing accountSid in start event`);
              ws.send(JSON.stringify({ event: 'error', message: 'Missing accountSid in start event' }));
              ws.close();
              return;
            }
            
            ws.sessionAgentConfig = agentConfig;
            currentLanguage = agentConfig.language || 'hi';
            console.log(`   ğŸŒ [LANGUAGE] Set to: ${currentLanguage}`);

            // Initialize call logger
            callLogger = new CallLogger(agentConfig.clientId || accountSid, mobile);
            console.log(`   ğŸ“ [CALL-LOG] Initialized for client: ${agentConfig.clientId}, mobile: ${mobile}`);

            await connectToDeepgram();
            
            // Use agent's firstMessage for greeting and log it
            const greeting = agentConfig.firstMessage || "Hello! How can I help you today?";
            console.log(`   ğŸ‘‹ [GREETING] "${greeting}"`);
            
            // Log the initial greeting
            if (callLogger) {
              callLogger.logAIResponse(greeting, currentLanguage);
            }
            
            const tts = new OptimizedSarvamTTSProcessor(currentLanguage, ws, streamSid, callLogger);
            await tts.synthesizeAndStream(greeting);
            break;
          }

          case "media":
            const mediaPayload = data.media?.payload;
            const mediaTimestamp = data.media?.timestamp;
            
            console.log(`   ğŸµ [EVENT-MEDIA] Media Details:`);
            console.log(`      ğŸ“¡ StreamSid: ${data.streamSid}`);
            console.log(`      ğŸ• Timestamp: ${mediaTimestamp}`);
            console.log(`      ğŸ“¦ Payload Length: ${mediaPayload ? mediaPayload.length : 0} chars`);
            console.log(`      ğŸ”¤ Payload Sample (first 100 chars): ${mediaPayload ? mediaPayload.substring(0, 100) + '...' : 'null'}`);
            
            if (mediaPayload) {
              const audioBuffer = Buffer.from(mediaPayload, "base64");
              console.log(`      ğŸ“Š Audio Buffer Size: ${audioBuffer.length} bytes`);
              
              if (deepgramWs && deepgramReady && deepgramWs.readyState === WebSocket.OPEN) {
                console.log(`      âœ… [DEEPGRAM] Sending audio to Deepgram`);
                deepgramWs.send(audioBuffer);
              } else {
                console.log(`      â³ [DEEPGRAM] Queueing audio (not ready yet)`);
                deepgramAudioQueue.push(audioBuffer);
              }
            } else {
              console.log(`      âŒ [MEDIA-ERROR] No payload in media event`);
            }
            break;

          case "stop":
            console.log(`   ğŸ“ [EVENT-STOP] Stream stopped`);
            console.log(`   ğŸ“‹ Stop Data:`, JSON.stringify(data, null, 2));
            
            // Save call log to database before closing
            if (callLogger) {
              try {
                const savedLog = await callLogger.saveToDatabase('medium'); // Default lead status
                console.log(`   ğŸ’¾ [CALL-LOG] Final save completed - ID: ${savedLog._id}`);
                
                // Print call statistics
                const stats = callLogger.getStats();
                console.log(`   ğŸ“Š [CALL-STATS] Duration: ${stats.duration}s, User: ${stats.userMessages}, AI: ${stats.aiResponses}, Languages: ${stats.languages.join(', ')}`);
              } catch (error) {
                console.error(`   âŒ [CALL-LOG] Failed to save final log: ${error.message}`);
              }
            }
            
            if (deepgramWs?.readyState === WebSocket.OPEN) {
              deepgramWs.close();
            }
            break;

          case "mark":
            console.log(`   ğŸ·ï¸  [EVENT-MARK] Mark Details:`);
            console.log(`   ğŸ“‹ Mark Data:`, JSON.stringify(data, null, 2));
            break;

          case "dtmf":
            console.log(`   ğŸ“± [EVENT-DTMF] DTMF Details:`);
            console.log(`      ğŸ”¢ Digit: ${data.dtmf?.digit}`);
            console.log(`   ğŸ“‹ DTMF Data:`, JSON.stringify(data, null, 2));
            break;

          default:
            console.log(`   â“ [EVENT-UNKNOWN] Unknown event: ${data.event}`);
            console.log(`   ğŸ“‹ Unknown Event Data:`, JSON.stringify(data, null, 2));
        }
        
        console.log(`â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”`);
        
      } catch (error) {
        console.error(`âŒ [WS-MESSAGE-ERROR] Error processing message: ${error.message}`);
        console.error(`âŒ [WS-MESSAGE-ERROR] Stack trace:`, error.stack);
        console.error(`âŒ [WS-MESSAGE-ERROR] Raw message:`, message.toString());
      }
    });

    // Enhanced connection cleanup with call logging
    ws.on("close", async (code, reason) => {
      console.log(`\nğŸ”— [WS-CLOSE] Connection Closed:`);
      console.log(`   ğŸ• Timestamp: ${new Date().toISOString()}`);
      console.log(`   ğŸ”¢ Close Code: ${code}`);
      console.log(`   ğŸ“ Close Reason: ${reason || 'No reason provided'}`);
      console.log(`   ğŸ“Š Connection Duration: ${Date.now() - (ws.connectedAt?.getTime() || Date.now())}ms`);
      
      // Save call log before cleanup if not already saved
      if (callLogger) {
        try {
          const savedLog = await callLogger.saveToDatabase('not_connected'); // Status for unexpected disconnection
          console.log(`   ğŸ’¾ [CALL-LOG] Emergency save completed - ID: ${savedLog._id}`);
        } catch (error) {
          console.error(`   âŒ [CALL-LOG] Emergency save failed: ${error.message}`);
        }
      }
      
      if (deepgramWs?.readyState === WebSocket.OPEN) {
        deepgramWs.close();
      }

      // Reset state
      streamSid = null;
      conversationHistory = [];
      isProcessing = false;
      userUtteranceBuffer = "";
      lastProcessedText = "";
      deepgramReady = false;
      deepgramAudioQueue = [];
      optimizedTTS = null;
      currentLanguage = undefined;
      processingRequestId = 0;
      callLogger = null;
      
      console.log(`â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”`);
    });

    ws.on("error", (error) => {
      console.log(`\nâŒ [WS-ERROR] WebSocket Error:`);
      console.log(`   ğŸ• Timestamp: ${new Date().toISOString()}`);
      console.log(`   ğŸ“ Error Message: ${error.message}`);
      console.log(`   ğŸ“‹ Error Stack:`, error.stack);
      console.log(`   ğŸ”§ Error Code: ${error.code}`);
      console.log(`   ğŸ“Š Error Type: ${error.type || 'Unknown'}`);
      console.log(`â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”`);
    });
  });
};

module.exports = { setupUnifiedVoiceServer };